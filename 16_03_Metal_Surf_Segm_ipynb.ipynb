{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/phbez/m_s/blob/main/16_03_Metal_Surf_Segm_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpfAq_3AqbLm"
      },
      "source": [
        "1. Importação das bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "O-ELVWXkAQCb"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tensorflow'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcv2\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Sequential, Model\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Conv2D, MaxPooling2D, Dense, Dropout, Flatten, BatchNormalization, Activation, Add, Input, AveragePooling2D\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow'"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten, BatchNormalization, Activation, Add, Input, AveragePooling2D\n",
        "from keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import regularizers\n",
        "\n",
        "import numpy as np\n",
        "import os, sys\n",
        "from scipy import ndimage\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "import scipy.stats\n",
        "\n",
        "from keras import applications, optimizers, Input\n",
        "from keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.utils.multiclass import unique_labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuuRcyf5zzcq"
      },
      "source": [
        "**1.1 Segment Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63QMM-SGz3rX",
        "outputId": "6f0e8f53-18e9-4fbd-b110-9a150c25228e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/facebookresearch/segment-anything.git\n",
            "  Cloning https://github.com/facebookresearch/segment-anything.git to c:\\users\\paulo\\appdata\\local\\temp\\pip-req-build-ak50pl86\n",
            "  Resolved https://github.com/facebookresearch/segment-anything.git to commit dca509fe793f601edb92606367a655c15ac00fdf\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/segment-anything.git 'C:\\Users\\Paulo\\AppData\\Local\\Temp\\pip-req-build-ak50pl86'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opencv-python in c:\\users\\paulo\\.ipython\\m_s\\ambienteph\\lib\\site-packages (4.11.0.86)\n",
            "Requirement already satisfied: pycocotools in c:\\users\\paulo\\.ipython\\m_s\\ambienteph\\lib\\site-packages (2.0.8)\n",
            "Requirement already satisfied: matplotlib in c:\\users\\paulo\\.ipython\\m_s\\ambienteph\\lib\\site-packages (3.10.1)\n",
            "Requirement already satisfied: onnxruntime in c:\\users\\paulo\\.ipython\\m_s\\ambienteph\\lib\\site-packages (1.21.0)\n",
            "Requirement already satisfied: onnx in c:\\users\\paulo\\.ipython\\m_s\\ambienteph\\lib\\site-packages (1.17.0)\n",
            "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\paulo\\.ipython\\m_s\\ambienteph\\lib\\site-packages (from opencv-python) (2.2.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\paulo\\.ipython\\m_s\\ambienteph\\lib\\site-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\paulo\\.ipython\\m_s\\ambienteph\\lib\\site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\paulo\\.ipython\\m_s\\ambienteph\\lib\\site-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\paulo\\.ipython\\m_s\\ambienteph\\lib\\site-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\paulo\\.ipython\\m_s\\ambienteph\\lib\\site-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in c:\\users\\paulo\\.ipython\\m_s\\ambienteph\\lib\\site-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\paulo\\.ipython\\m_s\\ambienteph\\lib\\site-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\paulo\\.ipython\\m_s\\ambienteph\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: coloredlogs in c:\\users\\paulo\\.ipython\\m_s\\ambienteph\\lib\\site-packages (from onnxruntime) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in c:\\users\\paulo\\.ipython\\m_s\\ambienteph\\lib\\site-packages (from onnxruntime) (25.2.10)\n",
            "Requirement already satisfied: protobuf in c:\\users\\paulo\\.ipython\\m_s\\ambienteph\\lib\\site-packages (from onnxruntime) (6.30.1)\n",
            "Requirement already satisfied: sympy in c:\\users\\paulo\\.ipython\\m_s\\ambienteph\\lib\\site-packages (from onnxruntime) (1.13.1)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\paulo\\.ipython\\m_s\\ambienteph\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\paulo\\.ipython\\m_s\\ambienteph\\lib\\site-packages (from coloredlogs->onnxruntime) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\paulo\\.ipython\\m_s\\ambienteph\\lib\\site-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Requirement already satisfied: pyreadline3 in c:\\users\\paulo\\.ipython\\m_s\\ambienteph\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime) (3.5.4)\n",
            "Checkpoint already exists at sam_models/sam_vit_b.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "J� existe uma subpasta ou um arquivo -p.\n",
            "Erro ao processar: -p.\n",
            "J� existe uma subpasta ou um arquivo sam_models.\n",
            "Erro ao processar: sam_models.\n"
          ]
        }
      ],
      "source": [
        "#####################################################################\n",
        "# SAM Model Setup\n",
        "!pip install git+https://github.com/facebookresearch/segment-anything.git\n",
        "!pip install opencv-python pycocotools matplotlib onnxruntime onnx\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import requests\n",
        "from segment_anything import sam_model_registry, SamPredictor, SamAutomaticMaskGenerator\n",
        "\n",
        "# Create a directory for the model\n",
        "!mkdir -p sam_models\n",
        "\n",
        "# Download SAM model checkpoint\n",
        "def download_sam_checkpoint(model_type='vit_b'):\n",
        "    checkpoint_urls = {\n",
        "        'vit_h': 'https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth',\n",
        "        'vit_l': 'https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth',\n",
        "        'vit_b': 'https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth'\n",
        "    }\n",
        "\n",
        "    checkpoint_path = f\"sam_models/sam_{model_type}.pth\"\n",
        "\n",
        "    if not os.path.exists(checkpoint_path):\n",
        "        print(f\"Downloading {model_type} checkpoint...\")\n",
        "        response = requests.get(checkpoint_urls[model_type])\n",
        "        with open(checkpoint_path, \"wb\") as f:\n",
        "            f.write(response.content)\n",
        "        print(f\"Downloaded {model_type} checkpoint to {checkpoint_path}\")\n",
        "    else:\n",
        "        print(f\"Checkpoint already exists at {checkpoint_path}\")\n",
        "\n",
        "    return checkpoint_path\n",
        "\n",
        "# Download and load the model\n",
        "model_type = 'vit_b'  # Use smaller model for faster processing\n",
        "checkpoint_path = download_sam_checkpoint(model_type)\n",
        "sam = sam_model_registry[model_type](checkpoint=checkpoint_path)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "sam.to(device)\n",
        "\n",
        "# Create SAM predictor and mask generator\n",
        "predictor = SamPredictor(sam)\n",
        "mask_generator = SamAutomaticMaskGenerator(\n",
        "    model=sam,\n",
        "    points_per_side=32,\n",
        "    pred_iou_thresh=0.85,\n",
        "    stability_score_thresh=0.9,\n",
        "    crop_n_layers=1,\n",
        "    crop_n_points_downscale_factor=2,\n",
        "    min_mask_region_area=100\n",
        ")\n",
        "#####################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVShPA5kEb1h"
      },
      "source": [
        "**2. Loading dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97FEPpcSBGiU",
        "outputId": "2ae9068e-93e7-432a-8c39-c4aceacf0a03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gdown in c:\\users\\paulo\\.ipython\\m_s\\ambienteph\\lib\\site-packages (5.2.0)\n",
            "Requirement already satisfied: pandas in c:\\users\\paulo\\.ipython\\m_s\\ambienteph\\lib\\site-packages (2.2.3)\n",
            "Requirement already satisfied: numpy in c:\\users\\paulo\\.ipython\\m_s\\ambienteph\\lib\\site-packages (2.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in c:\\users\\paulo\\.ipython\\m_s\\ambienteph\\lib\\site-packages (from gdown) (4.13.3)\n",
            "Requirement already satisfied: filelock in c:\\users\\paulo\\.ipython\\m_s\\ambienteph\\lib\\site-packages (from gdown) (3.18.0)\n",
            "Requirement already satisfied: requests[socks] in c:\\users\\paulo\\.ipython\\m_s\\ambienteph\\lib\\site-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in c:\\users\\paulo\\.ipython\\m_s\\ambienteph\\lib\\site-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\paulo\\.ipython\\m_s\\ambienteph\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\paulo\\.ipython\\m_s\\ambienteph\\lib\\site-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\paulo\\.ipython\\m_s\\ambienteph\\lib\\site-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\paulo\\.ipython\\m_s\\ambienteph\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in c:\\users\\paulo\\.ipython\\m_s\\ambienteph\\lib\\site-packages (from beautifulsoup4->gdown) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\paulo\\.ipython\\m_s\\ambienteph\\lib\\site-packages (from beautifulsoup4->gdown) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\paulo\\.ipython\\m_s\\ambienteph\\lib\\site-packages (from requests[socks]->gdown) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\paulo\\.ipython\\m_s\\ambienteph\\lib\\site-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\paulo\\.ipython\\m_s\\ambienteph\\lib\\site-packages (from requests[socks]->gdown) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\paulo\\.ipython\\m_s\\ambienteph\\lib\\site-packages (from requests[socks]->gdown) (2025.1.31)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\paulo\\.ipython\\m_s\\ambienteph\\lib\\site-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\paulo\\.ipython\\m_s\\ambienteph\\lib\\site-packages (from tqdm->gdown) (0.4.6)\n"
          ]
        },
        {
          "ename": "FileURLRetrievalError",
          "evalue": "Failed to retrieve file url:\n\n\tCannot retrieve the public link of the file. You may need to change\n\tthe permission to 'Anyone with the link', or have had many accesses.\n\tCheck FAQ in https://github.com/wkentaro/gdown?tab=readme-ov-file#faq.\n\nYou may still be able to access the file from the browser:\n\n\thttps://drive.google.com/uc?id=1XAt3LTujyxdU4EvBGRWbElfyUAByqoXw\n\nbut Gdown can't. Please check connections and permissions.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mFileURLRetrievalError\u001b[39m                     Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Paulo\\.ipython\\m_s\\ambienteph\\Lib\\site-packages\\gdown\\download.py:267\u001b[39m, in \u001b[36mdownload\u001b[39m\u001b[34m(url, output, quiet, proxy, speed, use_cookies, verify, id, fuzzy, resume, format, user_agent, log_messages)\u001b[39m\n\u001b[32m    266\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m     url = \u001b[43mget_url_from_gdrive_confirmation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m FileURLRetrievalError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Paulo\\.ipython\\m_s\\ambienteph\\Lib\\site-packages\\gdown\\download.py:55\u001b[39m, in \u001b[36mget_url_from_gdrive_confirmation\u001b[39m\u001b[34m(contents)\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m url:\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m FileURLRetrievalError(\n\u001b[32m     56\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot retrieve the public link of the file. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     57\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou may need to change the permission to \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     58\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mAnyone with the link\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, or have had many accesses. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     59\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCheck FAQ in https://github.com/wkentaro/gdown?tab=readme-ov-file#faq.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     60\u001b[39m     )\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m url\n",
            "\u001b[31mFileURLRetrievalError\u001b[39m: Cannot retrieve the public link of the file. You may need to change the permission to 'Anyone with the link', or have had many accesses. Check FAQ in https://github.com/wkentaro/gdown?tab=readme-ov-file#faq.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[31mFileURLRetrievalError\u001b[39m                     Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Download from Google Drive\u001b[39;00m\n\u001b[32m     19\u001b[39m url = \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mhttps://drive.google.com/uc?id=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[43mgdown\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquiet\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# If dataset is zipped, unzip it\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_path.endswith(\u001b[33m'\u001b[39m\u001b[33m.zip\u001b[39m\u001b[33m'\u001b[39m):\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Paulo\\.ipython\\m_s\\ambienteph\\Lib\\site-packages\\gdown\\download.py:278\u001b[39m, in \u001b[36mdownload\u001b[39m\u001b[34m(url, output, quiet, proxy, speed, use_cookies, verify, id, fuzzy, resume, format, user_agent, log_messages)\u001b[39m\n\u001b[32m    268\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m FileURLRetrievalError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    269\u001b[39m         message = (\n\u001b[32m    270\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mFailed to retrieve file url:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    271\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYou may still be able to access the file from the browser:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    276\u001b[39m             url_origin,\n\u001b[32m    277\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m278\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m FileURLRetrievalError(message)\n\u001b[32m    280\u001b[39m filename_from_url = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    281\u001b[39m last_modified_time = \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[31mFileURLRetrievalError\u001b[39m: Failed to retrieve file url:\n\n\tCannot retrieve the public link of the file. You may need to change\n\tthe permission to 'Anyone with the link', or have had many accesses.\n\tCheck FAQ in https://github.com/wkentaro/gdown?tab=readme-ov-file#faq.\n\nYou may still be able to access the file from the browser:\n\n\thttps://drive.google.com/uc?id=1XAt3LTujyxdU4EvBGRWbElfyUAByqoXw\n\nbut Gdown can't. Please check connections and permissions."
          ]
        }
      ],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "!pip install gdown pandas numpy\n",
        "\n",
        "import os\n",
        "import gdown\n",
        "\n",
        "# Create data directories\n",
        "DATA_DIR = os.path.join(os.getcwd(), 'data')\n",
        "DATASET_DIR = os.path.join(DATA_DIR, 'dataset')\n",
        "os.makedirs(DATASET_DIR, exist_ok=True)\n",
        "\n",
        "# Google Drive file ID (replace with your file ID)\n",
        "file_id = '1XAt3LTujyxdU4EvBGRWbElfyUAByqoXw'\n",
        "output_path = os.path.join(DATASET_DIR, 'dataset.zip')\n",
        "\n",
        "\n",
        "# Download from Google Drive\n",
        "url = f'https://drive.google.com/uc?id={file_id}'\n",
        "gdown.download(url, output_path, quiet=False)\n",
        "\n",
        "# If dataset is zipped, unzip it\n",
        "if output_path.endswith('.zip'):\n",
        "    import zipfile\n",
        "    with zipfile.ZipFile(output_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(DATASET_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Jif1DKcqC7n5"
      },
      "outputs": [],
      "source": [
        "folder='/content/drive/MyDrive/Pós-doutorado/Dataset_M_S_T_01'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJqawfUWEk61"
      },
      "source": [
        "3. Estruturando o dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "U_RoGeP5D-Wl",
        "outputId": "9b5b299e-a110-4a2c-bef9-97a2786eab85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Pós-doutorado/Dataset_M_S_T_01/Crazing\n",
            "/content/drive/MyDrive/Pós-doutorado/Dataset_M_S_T_01/Scratches\n",
            "/content/drive/MyDrive/Pós-doutorado/Dataset_M_S_T_01/Inclusion\n",
            "Processing images with SAM...\n",
            "Processing image 1/828\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-54d09159f44f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;31m# Process image with SAM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mprocessed_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_with_sam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# Normalize and add to dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-54d09159f44f>\u001b[0m in \u001b[0;36mprocess_with_sam\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# Generate masks for the image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mmasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# If no masks found, return the original image resized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/segment_anything/automatic_mask_generator.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;31m# Generate masks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0mmask_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;31m# Filter small disconnected regions and holes in masks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/segment_anything/automatic_mask_generator.py\u001b[0m in \u001b[0;36m_generate_masks\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMaskData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcrop_box\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrop_boxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_idxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m             \u001b[0mcrop_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_crop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrop_box\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrop_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/segment_anything/automatic_mask_generator.py\u001b[0m in \u001b[0;36m_process_crop\u001b[0;34m(self, image, crop_box, crop_layer_idx, orig_size)\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMaskData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoints_per_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoints_for_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0mbatch_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcropped_im_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrop_box\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/segment_anything/automatic_mask_generator.py\u001b[0m in \u001b[0;36m_process_batch\u001b[0;34m(self, points, im_size, crop_box, orig_size)\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0min_points\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformed_points\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0min_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_points\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0min_points\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m         masks, iou_preds, _ = self.predictor.predict_torch(\n\u001b[0m\u001b[1;32m    280\u001b[0m             \u001b[0min_points\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0min_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/segment_anything/predictor.py\u001b[0m in \u001b[0;36mpredict_torch\u001b[0;34m(self, point_coords, point_labels, boxes, mask_input, multimask_output, return_logits)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;31m# Predict masks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         low_res_masks, iou_predictions = self.model.mask_decoder(\n\u001b[0m\u001b[1;32m    230\u001b[0m             \u001b[0mimage_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0mimage_pe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dense_pe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/segment_anything/modeling/mask_decoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, image_embeddings, image_pe, sparse_prompt_embeddings, dense_prompt_embeddings, multimask_output)\u001b[0m\n\u001b[1;32m     92\u001b[0m           \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatched\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0mof\u001b[0m \u001b[0mmask\u001b[0m \u001b[0mquality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \"\"\"\n\u001b[0;32m---> 94\u001b[0;31m         masks, iou_pred = self.predict_masks(\n\u001b[0m\u001b[1;32m     95\u001b[0m             \u001b[0mimage_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_embeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mimage_pe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_pe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/segment_anything/modeling/mask_decoder.py\u001b[0m in \u001b[0;36mpredict_masks\u001b[0;34m(self, image_embeddings, image_pe, sparse_prompt_embeddings, dense_prompt_embeddings)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;31m# Upscale mask embeddings and predict masks using the mask tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0mupscaled_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_upscaling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0mhyper_in_list\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_mask_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/segment_anything/modeling/common.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#image resolution\n",
        "image_width = 48\n",
        "image_height = 48\n",
        "channels = 3 #rgb\n",
        "\n",
        "\n",
        "#####################################################################\n",
        "train_files = []\n",
        "classes_list = ['Crazing', 'Scratches', 'Inclusion']\n",
        "class_to_index = {cls: i for i, cls in enumerate(classes_list)}\n",
        "\n",
        "for cls in classes_list:\n",
        "    print(os.path.join(folder, cls))\n",
        "    onlyfiles = [f for f in os.listdir(os.path.join(folder, cls)) if os.path.isfile(os.path.join(folder, cls, f))]\n",
        "    for _file in onlyfiles:\n",
        "        train_files.append((os.path.join(folder, cls, _file), cls))\n",
        "\n",
        "# Function to process image with SAM\n",
        "def process_with_sam(image_path):\n",
        "    # Read the image\n",
        "    image = cv2.imread(image_path)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Generate masks for the image\n",
        "    masks = mask_generator.generate(image)\n",
        "\n",
        "    # If no masks found, return the original image resized\n",
        "    if not masks:\n",
        "        return cv2.resize(image, (image_width, image_height))\n",
        "\n",
        "    # Sort masks by area (largest first)\n",
        "    masks = sorted(masks, key=lambda x: x['area'], reverse=True)\n",
        "\n",
        "    # Get the largest mask (likely the main defect)\n",
        "    mask = masks[0]['segmentation']\n",
        "\n",
        "    # Apply the mask to the original image\n",
        "    masked_image = image.copy()\n",
        "    masked_image[~mask] = 0  # Set background to black\n",
        "\n",
        "    # Resize to the required dimensions\n",
        "    masked_image = cv2.resize(masked_image, (image_width, image_height))\n",
        "\n",
        "    return masked_image\n",
        "\n",
        "# Process and load the dataset\n",
        "dataset = np.zeros((len(train_files), image_height, image_width, channels), dtype=np.float32)\n",
        "y_dataset = []\n",
        "\n",
        "print(\"Processing images with SAM...\")\n",
        "for idx, (file_path, cls) in enumerate(train_files):\n",
        "    if idx % 10 == 0:\n",
        "        print(f\"Processing image {idx+1}/{len(train_files)}\")\n",
        "\n",
        "    # Process image with SAM\n",
        "    processed_image = process_with_sam(file_path)\n",
        "\n",
        "    # Normalize and add to dataset\n",
        "    dataset[idx] = processed_image / 255.0\n",
        "\n",
        "    # Add class label\n",
        "    y_dataset.append(class_to_index[cls])\n",
        "\n",
        "# Convert labels to categorical\n",
        "y_dataset = to_categorical(y_dataset, num_classes=len(classes_list))\n",
        "#####################################################################\n",
        "\n",
        "\n",
        "#train_files = []\n",
        "#for classes in ['Crazing', 'Scratches','Inclusion']:\n",
        "    #print(os.path.join(folder, classes))\n",
        "    #onlyfiles = [f for f in os.listdir(os.path.join(folder, classes)) if os.path.isfile(os.path.join(folder, classes, f ))]\n",
        "    #for _file in onlyfiles:\n",
        "        #train_files.append(_file)\n",
        "\n",
        "#dataset = np.ndarray(shape=(len(train_files), image_height, image_width, channels),\n",
        "                    # dtype=np.float32)\n",
        "#y_dataset = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IX_rk-gqH20K"
      },
      "outputs": [],
      "source": [
        "#i = 0\n",
        "#for classes in ['Crazing', 'Scratches','Inclusion']:\n",
        "    #onlyfiles = [f for f in os.listdir(os.path.join(folder, classes)) if os.path.isfile(os.path.join(folder, classes, f))]\n",
        "    #for _file in onlyfiles:\n",
        "        #img_path = os.path.join(folder, classes, _file)\n",
        "        #img = load_img(img_path, target_size=(image_height, image_width))\n",
        "        #x = img_to_array(img)\n",
        "        #dataset[i] = x\n",
        "        #mapping = {'Crazing': 0 , 'Scratches': 1, 'Inclusion': 2}\n",
        "        #y_dataset.append(mapping[classes])\n",
        "        #i += 1\n",
        "        #if i == 30000:\n",
        "        #    print(\"%d images to array\" % i)\n",
        "        #    break\n",
        "\n",
        "print(\"All images to array!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4PABmo4i083"
      },
      "source": [
        "6. Normalizando os dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMVib39IK1hK"
      },
      "outputs": [],
      "source": [
        "#Normalização\n",
        "#dataset = dataset.astype('float32')\n",
        "#dataset /= 255"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NXJK8iv97qB"
      },
      "source": [
        "**3.1 Visualize some examples with SAM segmentation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQFH06Im-GnU"
      },
      "outputs": [],
      "source": [
        "# Add this after your dataset loading to visualize some examples\n",
        "#####################################################################\n",
        "# Visualize some examples with SAM segmentation\n",
        "def visualize_sam_examples(num_examples=5):\n",
        "    plt.figure(figsize=(15, 5*num_examples))\n",
        "\n",
        "    for i in range(min(num_examples, len(train_files))):\n",
        "        # Get a random sample\n",
        "        idx = np.random.randint(0, len(train_files))\n",
        "        file_path, cls = train_files[idx]\n",
        "\n",
        "        # Original image\n",
        "        original = cv2.imread(file_path)\n",
        "        original = cv2.cvtColor(original, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Process with SAM\n",
        "        processed = process_with_sam(file_path)\n",
        "\n",
        "        # Display\n",
        "        plt.subplot(num_examples, 2, i*2+1)\n",
        "        plt.imshow(original)\n",
        "        plt.title(f\"Original - Class: {cls}\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(num_examples, 2, i*2+2)\n",
        "        plt.imshow(processed)\n",
        "        plt.title(f\"SAM Segmented - Class: {cls}\")\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Visualize some examples\n",
        "visualize_sam_examples()\n",
        "#####################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZ7Ad5VeK3EK"
      },
      "outputs": [],
      "source": [
        "classes = ['Crazing', 'Scratches', 'Inclusion']\n",
        "\n",
        "# Dicionário para armazenar o índice da primeira imagem de cada classe\n",
        "first_image_index = {}\n",
        "\n",
        "# Encontra o índice da primeira imagem de cada classe\n",
        "for i, label in enumerate(y_dataset):\n",
        "    if label not in first_image_index:\n",
        "        first_image_index[label] = i\n",
        "\n",
        "# Configura a grade para exibir as imagens\n",
        "num_classes = len(set(y_dataset))\n",
        "num_images_per_class = 1\n",
        "num_cols = num_classes\n",
        "num_rows = num_images_per_class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7jaYKd-La09"
      },
      "outputs": [],
      "source": [
        "# Cria uma figura com uma grade de subplots\n",
        "fig, axes = plt.subplots(num_rows, num_cols, figsize=(5, 15))\n",
        "\n",
        "# Loop através de cada classe\n",
        "for i in range(num_classes):\n",
        "    # Obtém o índice da primeira imagem da classe\n",
        "    idx = first_image_index[i]\n",
        "\n",
        "    # Obtém a imagem e converte para RGB\n",
        "    pixels = dataset[idx]\n",
        "\n",
        "    # Exibe a imagem no subplot correspondente\n",
        "    axes[i].imshow(pixels, cmap='Spectral')\n",
        "    axes[i].axis('off')\n",
        "\n",
        "    # Adiciona um título para o subplot com o rótulo\n",
        "    axes[i].set_title(f'{classes[i]}')\n",
        "\n",
        "# Exibe a figura\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgawN2E0qN2x"
      },
      "source": [
        "7. Normalizando o dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2v4Yj4vOcmw"
      },
      "outputs": [],
      "source": [
        "# categorical values\n",
        "n_classes = len(set(y_dataset))\n",
        "print(n_classes)\n",
        "\n",
        "y_dataset_ = to_categorical(y_dataset, n_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4krsLVb4OiXA"
      },
      "outputs": [],
      "source": [
        "# Ajusta o tamanho do dataset para coincidir com o tamanho menor entre dataset_trimmed e y_dataset_\n",
        "min_length = min(len(dataset), len(y_dataset_))\n",
        "dataset_trimmed = dataset[:min_length]\n",
        "y_dataset_trimmed = y_dataset_[:min_length]\n",
        "\n",
        "# Verifica se os tamanhos agora estão iguais\n",
        "assert len(dataset_trimmed) == len(y_dataset_trimmed), \"Os tamanhos ainda não são consistentes!\"\n",
        "\n",
        "# Dividindo em conjuntos de treino e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(dataset_trimmed, y_dataset_trimmed, test_size=0.2)\n",
        "\n",
        "print(\"Train set size: {}, Test set size: {}\".format(len(X_train), len(X_test)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nr-kbaYcOp1U"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils import resample\n",
        "\n",
        "# Criar listas vazias para armazenar as amostras balanceadas\n",
        "balanced_X_train = []\n",
        "balanced_y_train = []\n",
        "\n",
        "# Determinar o número de amostras na classe majoritária\n",
        "majority_samples = 5000\n",
        "\n",
        "# Iterar sobre cada classe\n",
        "for class_label in np.unique(y_train.argmax(axis=1)):\n",
        "    # Filtrar amostras pertencentes a essa classe\n",
        "    X_class = X_train[y_train.argmax(axis=1) == class_label]\n",
        "    y_class = y_train[y_train.argmax(axis=1) == class_label]\n",
        "\n",
        "    # Calcular o número de amostras na classe menos representada\n",
        "    minority_samples = len(X_class)\n",
        "\n",
        "    # Balancear as amostras aumentando a classe menos representada\n",
        "    balanced_X_class, balanced_y_class = resample(X_class, y_class,\n",
        "                                                  replace=True,\n",
        "                                                  n_samples=majority_samples,\n",
        "                                                  random_state=42)\n",
        "\n",
        "    # Adicionar amostras balanceadas à lista\n",
        "    balanced_X_train.extend(balanced_X_class)\n",
        "    balanced_y_train.extend(balanced_y_class)\n",
        "\n",
        "# Converter listas em arrays numpy\n",
        "balanced_X_train = np.array(balanced_X_train)\n",
        "balanced_y_train = np.array(balanced_y_train)\n",
        "\n",
        "# Embaralhar amostras\n",
        "shuffled_indices = np.arange(len(balanced_X_train))\n",
        "np.random.shuffle(shuffled_indices)\n",
        "balanced_X_train = balanced_X_train[shuffled_indices]\n",
        "balanced_y_train = balanced_y_train[shuffled_indices]\n",
        "\n",
        "# Verificar o tamanho dos conjuntos de dados balanceados\n",
        "print(\"Tamanho do conjunto de treinamento balanceado:\", len(balanced_X_train))\n",
        "print(\"Tamanho do conjunto de teste:\", len(X_test))\n",
        "\n",
        "for class_label in np.unique(balanced_y_train.argmax(axis=1)):\n",
        "    count = np.sum(balanced_y_train.argmax(axis=1) == class_label)\n",
        "    print(f\"Classe {class_label}: {count} amostras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZorQkRrSOwWP"
      },
      "outputs": [],
      "source": [
        "# Criar o modelo\n",
        "model = Sequential()\n",
        "\n",
        "model.add(BatchNormalization(input_shape=(image_height, image_width, 3)))\n",
        "model.add(Conv2D(512, (3,3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "model.add(Dropout(0.2))  # Adiciona a camada de dropout\n",
        "\n",
        "model.add(Conv2D(256, (3,3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "model.add(Dropout(0.2))  # Adiciona a camada de dropout\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dropout(0.2))  # Adiciona a camada de dropout\n",
        "#model.add(Dense(2, activation='softmax'))  # Especifica 'softmax' como a função de ativação\n",
        "model.add(Dense(3, activation='softmax'))  # Especifica 'softmax' como a função de ativação\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xo_RERv1RhSR"
      },
      "source": [
        "4. Compilando o modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpodQ283O6u4"
      },
      "outputs": [],
      "source": [
        "optimizer = Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78PbJ9QSPAwq"
      },
      "outputs": [],
      "source": [
        "# Treinando o modelo\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Configurar EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=15)\n",
        "\n",
        "# Treinar o modelo\n",
        "history = model.fit(balanced_X_train, balanced_y_train,validation_split= 0.2, epochs=100, callbacks=[early_stopping], batch_size=64)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knlJ3ziiPkFX"
      },
      "outputs": [],
      "source": [
        "val_accuracy = history.history['val_accuracy']\n",
        "\n",
        "mean_val_accuracy = np.mean(val_accuracy)\n",
        "\n",
        "print(\"Valor médio de acurácia:\", mean_val_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wP1qB1BKFO5f"
      },
      "source": [
        "5. Carregando o modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8jkc_ryPoP5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7v28nYJ0q0Y7"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive (add this line)\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMv2uBUMq5kl"
      },
      "outputs": [],
      "source": [
        "# START OF SAVING CODE\n",
        "# Save training history\n",
        "history_save = pd.DataFrame(history.history)\n",
        "#history_save.to_csv('datasetNormal.csv') # old: temporarily saving\n",
        "history_save.to_csv('/content/drive/MyDrive/Pós-doutorado/Dataset_M_S_T_01/The_Model_Metal_Surfaces_v01/datasetNormal.csv')  # Changed path to Google Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQFa4na5ukEi"
      },
      "source": [
        "**Save model structure in JSON file**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RLiMvuRPq4q"
      },
      "outputs": [],
      "source": [
        "model_json = model.to_json()\n",
        "#with open(\"metal_surface_model.json\", \"w\") as json_file: # old: temporarily saving\n",
        "with open(\"/content/drive/MyDrive/Pós-doutorado/Dataset_M_S_T_01/The_Model_Metal_Surfaces_v01/metal_surface_model.json\", \"w\") as json_file:  # Changed path\n",
        "    json_file.write(model_json)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNqDDb3ruayF"
      },
      "source": [
        "**Save complete model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1NHVdRLPtD_"
      },
      "outputs": [],
      "source": [
        "#model.save('modelosalvo.keras') # old: temporarily saving\n",
        "model.save('/content/drive/MyDrive/Pós-doutorado/Dataset_M_S_T_01/The_Model_Metal_Surfaces_v01/modelosalvo.keras')  # Changed path\n",
        "# END OF SAVING CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xq0Kctrnu33p"
      },
      "source": [
        "**Load model (for verification or immediate use)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VnZg02iEPvam"
      },
      "outputs": [],
      "source": [
        "from keras.models import load_model\n",
        "modelo_carregado = load_model('/content/drive/MyDrive/Pós-doutorado/Dataset_M_S_T_01/The_Model_Metal_Surfaces_v01/modelosalvo.keras')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIv4CoHlvckk"
      },
      "source": [
        "**Load history**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LpmHXjmgPxyV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "history = pd.read_csv('/content/drive/MyDrive/Pós-doutorado/Dataset_M_S_T_01/The_Model_Metal_Surfaces_v01/datasetNormal.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXD1EO4ZFXLp"
      },
      "source": [
        "6. Plotando o modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R91qUmtGP16D"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history['loss'])\n",
        "plt.plot(history['val_loss'])\n",
        "\n",
        "\n",
        "plt.ylabel('Perda')\n",
        "plt.xlabel('Época')\n",
        "plt.legend(['Treinamento', 'Validação'], loc = 'upper right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ySD4TmTP33L"
      },
      "outputs": [],
      "source": [
        "plt.plot(history['accuracy'])\n",
        "plt.plot(history['val_accuracy'])\n",
        "plt.title('Acurácia')\n",
        "plt.ylabel('Acurácia')\n",
        "plt.xlabel('Épocas')\n",
        "plt.legend(['Treinamento', 'Validação'], loc='upper left')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvrrbmVjFekv"
      },
      "source": [
        "7. Salvando o modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZKAgvZsP605"
      },
      "outputs": [],
      "source": [
        "preds = modelo_carregado.predict(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-72gQj5DP9X6"
      },
      "outputs": [],
      "source": [
        "#Função da Matriz de Confusão\n",
        "\n",
        "def plot_confusion_matrix(\n",
        "        cm,\n",
        "        classes,\n",
        "        normalize=False,\n",
        "        title='Confusion matrix',\n",
        "        cmap=plt.cm.Blues\n",
        "    ):\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    #plt.title(title)\n",
        "    #plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('Classe real')\n",
        "    plt.xlabel('Classe predita')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nTqr20FP-PP"
      },
      "outputs": [],
      "source": [
        "#Gráfico da Matriz de Confusão\n",
        "\n",
        "y_test_ = [np.argmax(x) for x in y_test]\n",
        "preds_ = [np.argmax(x) for x in preds]\n",
        "\n",
        "cm = confusion_matrix(y_test_, preds_)\n",
        "plot_confusion_matrix(cm, classes=['Crazing', 'Scratches','Inclusion'], title='Confusion matrix')\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54fo7piqQDKp"
      },
      "outputs": [],
      "source": [
        "#Resultado das predições\n",
        "\n",
        "n = 4\n",
        "for t in range(4):\n",
        "    plt.figure(figsize=(10,10))\n",
        "    for i in range(n*t, n*(t+1)):\n",
        "        plt.subplot(1, n, i + 1 - n*t)\n",
        "        plt.imshow(cv2.cvtColor(X_test[i], cv2.COLOR_BGR2RGB), cmap='gray')\n",
        "        plt.title('Real: {}\\nPredito: {}'.format(classes[np.argmax(y_test[i])], classes[np.argmax(preds[i])]))\n",
        "        plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KUGeaIhLFQ0"
      },
      "source": [
        "# **Using the model to classify images**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJcNKH5GLStx"
      },
      "source": [
        "**1. Load Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYTpMv0TLLwn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from google.colab import drive\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.image import img_to_array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGguXh0iLZe5"
      },
      "source": [
        "**2. Google Drive and Path**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kpe_j-FfLcqR"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load the pre-trained model\n",
        "model_path = '/content/drive/MyDrive/Pós-doutorado/Dataset_M_S_T_01/The_Model_Metal_Surfaces_v01/modelosalvo.keras' #'/content/drive/MyDrive/modelosalvo.keras'  # Update this path to match your model's location\n",
        "model = load_model(model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nf7Tfun5L4ug"
      },
      "source": [
        "**3. Labels**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2pLa4HeMFyt"
      },
      "outputs": [],
      "source": [
        "# Define class labels (update these to match your model's classes)\n",
        "class_labels = ['Crazing', 'Scratches','Inclusion']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPPgXPCGMTT3"
      },
      "source": [
        "**4. Preprocessing images**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rU_xuovdMeJw"
      },
      "outputs": [],
      "source": [
        "def preprocess_image(image_path, target_size=(48, 48)):\n",
        "    \"\"\"Preprocess image for model input\"\"\"\n",
        "    img = Image.open(image_path).convert('RGB')\n",
        "    img = img.resize(target_size)\n",
        "    img_array = img_to_array(img)\n",
        "    img_array = img_array / 255.0  # Normalize pixel values\n",
        "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
        "    return img, img_array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7S6wwMdQO_Z"
      },
      "source": [
        "5. **Classify Image**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-oMDe8aQQ_s"
      },
      "outputs": [],
      "source": [
        "def classify_image(image_path):\n",
        "    \"\"\"Classify an image using the pre-trained model\"\"\"\n",
        "    # Preprocess image\n",
        "    original_img, processed_img = preprocess_image(image_path)\n",
        "\n",
        "    # Get prediction\n",
        "    prediction = model.predict(processed_img)\n",
        "    predicted_class_index = np.argmax(prediction[0])\n",
        "    predicted_class = class_labels[predicted_class_index]\n",
        "    confidence = prediction[0][predicted_class_index] * 100\n",
        "\n",
        "    # Display results\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(original_img)\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"This image is classified as: {predicted_class.upper()}\\nConfidence: {confidence:.2f}%\",\n",
        "              fontsize=16, pad=20)\n",
        "    plt.show()\n",
        "\n",
        "    return predicted_class, confidence\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvF9OD4eUkE_"
      },
      "source": [
        "**6. Example Usage**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-DK6mcqhUhKI"
      },
      "outputs": [],
      "source": [
        "# Example usage\n",
        "def classify_image_from_drive(relative_path):\n",
        "    \"\"\"Classify an image from Google Drive by providing a path relative to MyDrive\"\"\"\n",
        "    full_path = f'/content/drive/MyDrive/Pós-doutorado/Dataset_M_S_T_01/Crazing_test_image/{relative_path}'\n",
        "    if not os.path.exists(full_path):\n",
        "        print(f\"Error: File not found at {full_path}\")\n",
        "        return\n",
        "\n",
        "    print(f\"Processing image: {relative_path}\")\n",
        "    predicted_class, confidence = classify_image(full_path)\n",
        "    print(f\"Classification result: {predicted_class}\")\n",
        "    print(f\"Confidence: {confidence:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ro4P1PNKQmR7"
      },
      "source": [
        "**7. Function to let user input path**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97lga1ZPVV0V"
      },
      "outputs": [],
      "source": [
        "# Function to let user input a path\n",
        "def classify_from_user_input():\n",
        "    image_path = input(\"Enter the path to your image relative to Google Drive root (e.g., 'metal_surface_model/In_1.bmp'): \")\n",
        "    classify_image_from_drive(image_path)\n",
        "\n",
        "# Run the interactive function\n",
        "classify_from_user_input()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ambienteph",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
