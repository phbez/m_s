{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/phbez/m_s/blob/main/16_03_Metal_Surf_Segm_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpfAq_3AqbLm"
      },
      "source": [
        "1. Importação das bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "O-ELVWXkAQCb"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten, BatchNormalization, Activation, Add, Input, AveragePooling2D\n",
        "from keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import regularizers\n",
        "\n",
        "import numpy as np\n",
        "import os, sys\n",
        "from scipy import ndimage\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "import scipy.stats\n",
        "import tensorflow as tf\n",
        "from keras import applications, optimizers, Input\n",
        "from keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.utils.multiclass import unique_labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.1 Segment Model**"
      ],
      "metadata": {
        "id": "HuuRcyf5zzcq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#####################################################################\n",
        "# SAM Model Setup\n",
        "!pip install git+https://github.com/facebookresearch/segment-anything.git\n",
        "!pip install opencv-python pycocotools matplotlib onnxruntime onnx\n",
        "\n",
        "import torch\n",
        "import requests\n",
        "from segment_anything import sam_model_registry, SamPredictor, SamAutomaticMaskGenerator\n",
        "\n",
        "# Create a directory for the model\n",
        "!mkdir -p sam_models\n",
        "\n",
        "# Download SAM model checkpoint\n",
        "def download_sam_checkpoint(model_type='vit_b'):\n",
        "    checkpoint_urls = {\n",
        "        'vit_h': 'https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth',\n",
        "        'vit_l': 'https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth',\n",
        "        'vit_b': 'https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth'\n",
        "    }\n",
        "\n",
        "    checkpoint_path = f\"sam_models/sam_{model_type}.pth\"\n",
        "\n",
        "    if not os.path.exists(checkpoint_path):\n",
        "        print(f\"Downloading {model_type} checkpoint...\")\n",
        "        response = requests.get(checkpoint_urls[model_type])\n",
        "        with open(checkpoint_path, \"wb\") as f:\n",
        "            f.write(response.content)\n",
        "        print(f\"Downloaded {model_type} checkpoint to {checkpoint_path}\")\n",
        "    else:\n",
        "        print(f\"Checkpoint already exists at {checkpoint_path}\")\n",
        "\n",
        "    return checkpoint_path\n",
        "\n",
        "# Download and load the model\n",
        "model_type = 'vit_b'  # Use smaller model for faster processing\n",
        "checkpoint_path = download_sam_checkpoint(model_type)\n",
        "sam = sam_model_registry[model_type](checkpoint=checkpoint_path)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "sam.to(device)\n",
        "\n",
        "# Create SAM predictor and mask generator\n",
        "predictor = SamPredictor(sam)\n",
        "mask_generator = SamAutomaticMaskGenerator(\n",
        "    model=sam,\n",
        "    points_per_side=32,\n",
        "    pred_iou_thresh=0.85,\n",
        "    stability_score_thresh=0.9,\n",
        "    crop_n_layers=1,\n",
        "    crop_n_points_downscale_factor=2,\n",
        "    min_mask_region_area=100\n",
        ")\n",
        "#####################################################################"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63QMM-SGz3rX",
        "outputId": "6f0e8f53-18e9-4fbd-b110-9a150c25228e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/facebookresearch/segment-anything.git\n",
            "  Cloning https://github.com/facebookresearch/segment-anything.git to /tmp/pip-req-build-ox3o88wx\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/segment-anything.git /tmp/pip-req-build-ox3o88wx\n",
            "  Resolved https://github.com/facebookresearch/segment-anything.git to commit dca509fe793f601edb92606367a655c15ac00fdf\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: segment_anything\n",
            "  Building wheel for segment_anything (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for segment_anything: filename=segment_anything-1.0-py3-none-any.whl size=36592 sha256=2a68dda3f58c62a044b29cf64a071d9ea5b6e45f5718fd20a0496f4d7d786ad2\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-uwva0erf/wheels/15/d7/bd/05f5f23b7dcbe70cbc6783b06f12143b0cf1a5da5c7b52dcc5\n",
            "Successfully built segment_anything\n",
            "Installing collected packages: segment_anything\n",
            "Successfully installed segment_anything-1.0\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.11/dist-packages (2.0.8)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.21.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting onnx\n",
            "  Downloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (1.26.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Collecting coloredlogs (from onnxruntime)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (4.25.6)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (1.13.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Downloading onnxruntime-1.21.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m87.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnx, humanfriendly, coloredlogs, onnxruntime\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnx-1.17.0 onnxruntime-1.21.0\n",
            "Downloading vit_b checkpoint...\n",
            "Downloaded vit_b checkpoint to sam_models/sam_vit_b.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVShPA5kEb1h"
      },
      "source": [
        "**2. Loading dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97FEPpcSBGiU",
        "outputId": "2ae9068e-93e7-432a-8c39-c4aceacf0a03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Jif1DKcqC7n5"
      },
      "outputs": [],
      "source": [
        "folder='/content/drive/MyDrive/Pós-doutorado/Dataset_M_S_T_01'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJqawfUWEk61"
      },
      "source": [
        "3. Estruturando o dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "U_RoGeP5D-Wl",
        "outputId": "9b5b299e-a110-4a2c-bef9-97a2786eab85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Pós-doutorado/Dataset_M_S_T_01/Crazing\n",
            "/content/drive/MyDrive/Pós-doutorado/Dataset_M_S_T_01/Scratches\n",
            "/content/drive/MyDrive/Pós-doutorado/Dataset_M_S_T_01/Inclusion\n",
            "Processing images with SAM...\n",
            "Processing image 1/828\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-54d09159f44f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;31m# Process image with SAM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mprocessed_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_with_sam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# Normalize and add to dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-54d09159f44f>\u001b[0m in \u001b[0;36mprocess_with_sam\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# Generate masks for the image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mmasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# If no masks found, return the original image resized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/segment_anything/automatic_mask_generator.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;31m# Generate masks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0mmask_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;31m# Filter small disconnected regions and holes in masks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/segment_anything/automatic_mask_generator.py\u001b[0m in \u001b[0;36m_generate_masks\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMaskData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcrop_box\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrop_boxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_idxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m             \u001b[0mcrop_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_crop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrop_box\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrop_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/segment_anything/automatic_mask_generator.py\u001b[0m in \u001b[0;36m_process_crop\u001b[0;34m(self, image, crop_box, crop_layer_idx, orig_size)\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMaskData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoints_per_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoints_for_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0mbatch_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcropped_im_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrop_box\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/segment_anything/automatic_mask_generator.py\u001b[0m in \u001b[0;36m_process_batch\u001b[0;34m(self, points, im_size, crop_box, orig_size)\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0min_points\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformed_points\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0min_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_points\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0min_points\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m         masks, iou_preds, _ = self.predictor.predict_torch(\n\u001b[0m\u001b[1;32m    280\u001b[0m             \u001b[0min_points\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0min_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/segment_anything/predictor.py\u001b[0m in \u001b[0;36mpredict_torch\u001b[0;34m(self, point_coords, point_labels, boxes, mask_input, multimask_output, return_logits)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;31m# Predict masks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         low_res_masks, iou_predictions = self.model.mask_decoder(\n\u001b[0m\u001b[1;32m    230\u001b[0m             \u001b[0mimage_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0mimage_pe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dense_pe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/segment_anything/modeling/mask_decoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, image_embeddings, image_pe, sparse_prompt_embeddings, dense_prompt_embeddings, multimask_output)\u001b[0m\n\u001b[1;32m     92\u001b[0m           \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatched\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0mof\u001b[0m \u001b[0mmask\u001b[0m \u001b[0mquality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \"\"\"\n\u001b[0;32m---> 94\u001b[0;31m         masks, iou_pred = self.predict_masks(\n\u001b[0m\u001b[1;32m     95\u001b[0m             \u001b[0mimage_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_embeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mimage_pe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_pe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/segment_anything/modeling/mask_decoder.py\u001b[0m in \u001b[0;36mpredict_masks\u001b[0;34m(self, image_embeddings, image_pe, sparse_prompt_embeddings, dense_prompt_embeddings)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;31m# Upscale mask embeddings and predict masks using the mask tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0mupscaled_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_upscaling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0mhyper_in_list\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_mask_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/segment_anything/modeling/common.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#image resolution\n",
        "image_width = 48\n",
        "image_height = 48\n",
        "channels = 3 #rgb\n",
        "\n",
        "\n",
        "#####################################################################\n",
        "train_files = []\n",
        "classes_list = ['Crazing', 'Scratches', 'Inclusion']\n",
        "class_to_index = {cls: i for i, cls in enumerate(classes_list)}\n",
        "\n",
        "for cls in classes_list:\n",
        "    print(os.path.join(folder, cls))\n",
        "    onlyfiles = [f for f in os.listdir(os.path.join(folder, cls)) if os.path.isfile(os.path.join(folder, cls, f))]\n",
        "    for _file in onlyfiles:\n",
        "        train_files.append((os.path.join(folder, cls, _file), cls))\n",
        "\n",
        "# Function to process image with SAM\n",
        "def process_with_sam(image_path):\n",
        "    # Read the image\n",
        "    image = cv2.imread(image_path)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Generate masks for the image\n",
        "    masks = mask_generator.generate(image)\n",
        "\n",
        "    # If no masks found, return the original image resized\n",
        "    if not masks:\n",
        "        return cv2.resize(image, (image_width, image_height))\n",
        "\n",
        "    # Sort masks by area (largest first)\n",
        "    masks = sorted(masks, key=lambda x: x['area'], reverse=True)\n",
        "\n",
        "    # Get the largest mask (likely the main defect)\n",
        "    mask = masks[0]['segmentation']\n",
        "\n",
        "    # Apply the mask to the original image\n",
        "    masked_image = image.copy()\n",
        "    masked_image[~mask] = 0  # Set background to black\n",
        "\n",
        "    # Resize to the required dimensions\n",
        "    masked_image = cv2.resize(masked_image, (image_width, image_height))\n",
        "\n",
        "    return masked_image\n",
        "\n",
        "# Process and load the dataset\n",
        "dataset = np.zeros((len(train_files), image_height, image_width, channels), dtype=np.float32)\n",
        "y_dataset = []\n",
        "\n",
        "print(\"Processing images with SAM...\")\n",
        "for idx, (file_path, cls) in enumerate(train_files):\n",
        "    if idx % 10 == 0:\n",
        "        print(f\"Processing image {idx+1}/{len(train_files)}\")\n",
        "\n",
        "    # Process image with SAM\n",
        "    processed_image = process_with_sam(file_path)\n",
        "\n",
        "    # Normalize and add to dataset\n",
        "    dataset[idx] = processed_image / 255.0\n",
        "\n",
        "    # Add class label\n",
        "    y_dataset.append(class_to_index[cls])\n",
        "\n",
        "# Convert labels to categorical\n",
        "y_dataset = to_categorical(y_dataset, num_classes=len(classes_list))\n",
        "#####################################################################\n",
        "\n",
        "\n",
        "#train_files = []\n",
        "#for classes in ['Crazing', 'Scratches','Inclusion']:\n",
        "    #print(os.path.join(folder, classes))\n",
        "    #onlyfiles = [f for f in os.listdir(os.path.join(folder, classes)) if os.path.isfile(os.path.join(folder, classes, f ))]\n",
        "    #for _file in onlyfiles:\n",
        "        #train_files.append(_file)\n",
        "\n",
        "#dataset = np.ndarray(shape=(len(train_files), image_height, image_width, channels),\n",
        "                    # dtype=np.float32)\n",
        "#y_dataset = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IX_rk-gqH20K"
      },
      "outputs": [],
      "source": [
        "#i = 0\n",
        "#for classes in ['Crazing', 'Scratches','Inclusion']:\n",
        "    #onlyfiles = [f for f in os.listdir(os.path.join(folder, classes)) if os.path.isfile(os.path.join(folder, classes, f))]\n",
        "    #for _file in onlyfiles:\n",
        "        #img_path = os.path.join(folder, classes, _file)\n",
        "        #img = load_img(img_path, target_size=(image_height, image_width))\n",
        "        #x = img_to_array(img)\n",
        "        #dataset[i] = x\n",
        "        #mapping = {'Crazing': 0 , 'Scratches': 1, 'Inclusion': 2}\n",
        "        #y_dataset.append(mapping[classes])\n",
        "        #i += 1\n",
        "        #if i == 30000:\n",
        "        #    print(\"%d images to array\" % i)\n",
        "        #    break\n",
        "\n",
        "print(\"All images to array!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4PABmo4i083"
      },
      "source": [
        "6. Normalizando os dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMVib39IK1hK"
      },
      "outputs": [],
      "source": [
        "#Normalização\n",
        "#dataset = dataset.astype('float32')\n",
        "#dataset /= 255"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.1 Visualize some examples with SAM segmentation**"
      ],
      "metadata": {
        "id": "1NXJK8iv97qB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add this after your dataset loading to visualize some examples\n",
        "#####################################################################\n",
        "# Visualize some examples with SAM segmentation\n",
        "def visualize_sam_examples(num_examples=5):\n",
        "    plt.figure(figsize=(15, 5*num_examples))\n",
        "\n",
        "    for i in range(min(num_examples, len(train_files))):\n",
        "        # Get a random sample\n",
        "        idx = np.random.randint(0, len(train_files))\n",
        "        file_path, cls = train_files[idx]\n",
        "\n",
        "        # Original image\n",
        "        original = cv2.imread(file_path)\n",
        "        original = cv2.cvtColor(original, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Process with SAM\n",
        "        processed = process_with_sam(file_path)\n",
        "\n",
        "        # Display\n",
        "        plt.subplot(num_examples, 2, i*2+1)\n",
        "        plt.imshow(original)\n",
        "        plt.title(f\"Original - Class: {cls}\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(num_examples, 2, i*2+2)\n",
        "        plt.imshow(processed)\n",
        "        plt.title(f\"SAM Segmented - Class: {cls}\")\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Visualize some examples\n",
        "visualize_sam_examples()\n",
        "#####################################################################"
      ],
      "metadata": {
        "id": "TQFH06Im-GnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZ7Ad5VeK3EK"
      },
      "outputs": [],
      "source": [
        "classes = ['Crazing', 'Scratches', 'Inclusion']\n",
        "\n",
        "# Dicionário para armazenar o índice da primeira imagem de cada classe\n",
        "first_image_index = {}\n",
        "\n",
        "# Encontra o índice da primeira imagem de cada classe\n",
        "for i, label in enumerate(y_dataset):\n",
        "    if label not in first_image_index:\n",
        "        first_image_index[label] = i\n",
        "\n",
        "# Configura a grade para exibir as imagens\n",
        "num_classes = len(set(y_dataset))\n",
        "num_images_per_class = 1\n",
        "num_cols = num_classes\n",
        "num_rows = num_images_per_class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7jaYKd-La09"
      },
      "outputs": [],
      "source": [
        "# Cria uma figura com uma grade de subplots\n",
        "fig, axes = plt.subplots(num_rows, num_cols, figsize=(5, 15))\n",
        "\n",
        "# Loop através de cada classe\n",
        "for i in range(num_classes):\n",
        "    # Obtém o índice da primeira imagem da classe\n",
        "    idx = first_image_index[i]\n",
        "\n",
        "    # Obtém a imagem e converte para RGB\n",
        "    pixels = dataset[idx]\n",
        "\n",
        "    # Exibe a imagem no subplot correspondente\n",
        "    axes[i].imshow(pixels, cmap='Spectral')\n",
        "    axes[i].axis('off')\n",
        "\n",
        "    # Adiciona um título para o subplot com o rótulo\n",
        "    axes[i].set_title(f'{classes[i]}')\n",
        "\n",
        "# Exibe a figura\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgawN2E0qN2x"
      },
      "source": [
        "7. Normalizando o dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2v4Yj4vOcmw"
      },
      "outputs": [],
      "source": [
        "# categorical values\n",
        "n_classes = len(set(y_dataset))\n",
        "print(n_classes)\n",
        "\n",
        "y_dataset_ = to_categorical(y_dataset, n_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4krsLVb4OiXA"
      },
      "outputs": [],
      "source": [
        "# Ajusta o tamanho do dataset para coincidir com o tamanho menor entre dataset_trimmed e y_dataset_\n",
        "min_length = min(len(dataset), len(y_dataset_))\n",
        "dataset_trimmed = dataset[:min_length]\n",
        "y_dataset_trimmed = y_dataset_[:min_length]\n",
        "\n",
        "# Verifica se os tamanhos agora estão iguais\n",
        "assert len(dataset_trimmed) == len(y_dataset_trimmed), \"Os tamanhos ainda não são consistentes!\"\n",
        "\n",
        "# Dividindo em conjuntos de treino e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(dataset_trimmed, y_dataset_trimmed, test_size=0.2)\n",
        "\n",
        "print(\"Train set size: {}, Test set size: {}\".format(len(X_train), len(X_test)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nr-kbaYcOp1U"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils import resample\n",
        "\n",
        "# Criar listas vazias para armazenar as amostras balanceadas\n",
        "balanced_X_train = []\n",
        "balanced_y_train = []\n",
        "\n",
        "# Determinar o número de amostras na classe majoritária\n",
        "majority_samples = 5000\n",
        "\n",
        "# Iterar sobre cada classe\n",
        "for class_label in np.unique(y_train.argmax(axis=1)):\n",
        "    # Filtrar amostras pertencentes a essa classe\n",
        "    X_class = X_train[y_train.argmax(axis=1) == class_label]\n",
        "    y_class = y_train[y_train.argmax(axis=1) == class_label]\n",
        "\n",
        "    # Calcular o número de amostras na classe menos representada\n",
        "    minority_samples = len(X_class)\n",
        "\n",
        "    # Balancear as amostras aumentando a classe menos representada\n",
        "    balanced_X_class, balanced_y_class = resample(X_class, y_class,\n",
        "                                                  replace=True,\n",
        "                                                  n_samples=majority_samples,\n",
        "                                                  random_state=42)\n",
        "\n",
        "    # Adicionar amostras balanceadas à lista\n",
        "    balanced_X_train.extend(balanced_X_class)\n",
        "    balanced_y_train.extend(balanced_y_class)\n",
        "\n",
        "# Converter listas em arrays numpy\n",
        "balanced_X_train = np.array(balanced_X_train)\n",
        "balanced_y_train = np.array(balanced_y_train)\n",
        "\n",
        "# Embaralhar amostras\n",
        "shuffled_indices = np.arange(len(balanced_X_train))\n",
        "np.random.shuffle(shuffled_indices)\n",
        "balanced_X_train = balanced_X_train[shuffled_indices]\n",
        "balanced_y_train = balanced_y_train[shuffled_indices]\n",
        "\n",
        "# Verificar o tamanho dos conjuntos de dados balanceados\n",
        "print(\"Tamanho do conjunto de treinamento balanceado:\", len(balanced_X_train))\n",
        "print(\"Tamanho do conjunto de teste:\", len(X_test))\n",
        "\n",
        "for class_label in np.unique(balanced_y_train.argmax(axis=1)):\n",
        "    count = np.sum(balanced_y_train.argmax(axis=1) == class_label)\n",
        "    print(f\"Classe {class_label}: {count} amostras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZorQkRrSOwWP"
      },
      "outputs": [],
      "source": [
        "# Criar o modelo\n",
        "model = Sequential()\n",
        "\n",
        "model.add(BatchNormalization(input_shape=(image_height, image_width, 3)))\n",
        "model.add(Conv2D(512, (3,3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "model.add(Dropout(0.2))  # Adiciona a camada de dropout\n",
        "\n",
        "model.add(Conv2D(256, (3,3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "model.add(Dropout(0.2))  # Adiciona a camada de dropout\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dropout(0.2))  # Adiciona a camada de dropout\n",
        "#model.add(Dense(2, activation='softmax'))  # Especifica 'softmax' como a função de ativação\n",
        "model.add(Dense(3, activation='softmax'))  # Especifica 'softmax' como a função de ativação\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xo_RERv1RhSR"
      },
      "source": [
        "4. Compilando o modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpodQ283O6u4"
      },
      "outputs": [],
      "source": [
        "optimizer = Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78PbJ9QSPAwq"
      },
      "outputs": [],
      "source": [
        "# Treinando o modelo\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Configurar EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=15)\n",
        "\n",
        "# Treinar o modelo\n",
        "history = model.fit(balanced_X_train, balanced_y_train,validation_split= 0.2, epochs=100, callbacks=[early_stopping], batch_size=64)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knlJ3ziiPkFX"
      },
      "outputs": [],
      "source": [
        "val_accuracy = history.history['val_accuracy']\n",
        "\n",
        "mean_val_accuracy = np.mean(val_accuracy)\n",
        "\n",
        "print(\"Valor médio de acurácia:\", mean_val_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wP1qB1BKFO5f"
      },
      "source": [
        "5. Carregando o modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8jkc_ryPoP5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive (add this line)\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "7v28nYJ0q0Y7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# START OF SAVING CODE\n",
        "# Save training history\n",
        "history_save = pd.DataFrame(history.history)\n",
        "#history_save.to_csv('datasetNormal.csv') # old: temporarily saving\n",
        "history_save.to_csv('/content/drive/MyDrive/Pós-doutorado/Dataset_M_S_T_01/The_Model_Metal_Surfaces_v01/datasetNormal.csv')  # Changed path to Google Drive"
      ],
      "metadata": {
        "id": "TMv2uBUMq5kl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Save model structure in JSON file**"
      ],
      "metadata": {
        "id": "wQFa4na5ukEi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RLiMvuRPq4q"
      },
      "outputs": [],
      "source": [
        "model_json = model.to_json()\n",
        "#with open(\"metal_surface_model.json\", \"w\") as json_file: # old: temporarily saving\n",
        "with open(\"/content/drive/MyDrive/Pós-doutorado/Dataset_M_S_T_01/The_Model_Metal_Surfaces_v01/metal_surface_model.json\", \"w\") as json_file:  # Changed path\n",
        "    json_file.write(model_json)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Save complete model**"
      ],
      "metadata": {
        "id": "KNqDDb3ruayF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1NHVdRLPtD_"
      },
      "outputs": [],
      "source": [
        "#model.save('modelosalvo.keras') # old: temporarily saving\n",
        "model.save('/content/drive/MyDrive/Pós-doutorado/Dataset_M_S_T_01/The_Model_Metal_Surfaces_v01/modelosalvo.keras')  # Changed path\n",
        "# END OF SAVING CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load model (for verification or immediate use)**"
      ],
      "metadata": {
        "id": "Xq0Kctrnu33p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VnZg02iEPvam"
      },
      "outputs": [],
      "source": [
        "from keras.models import load_model\n",
        "modelo_carregado = load_model('/content/drive/MyDrive/Pós-doutorado/Dataset_M_S_T_01/The_Model_Metal_Surfaces_v01/modelosalvo.keras')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load history**"
      ],
      "metadata": {
        "id": "TIv4CoHlvckk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LpmHXjmgPxyV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "history = pd.read_csv('/content/drive/MyDrive/Pós-doutorado/Dataset_M_S_T_01/The_Model_Metal_Surfaces_v01/datasetNormal.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXD1EO4ZFXLp"
      },
      "source": [
        "6. Plotando o modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R91qUmtGP16D"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history['loss'])\n",
        "plt.plot(history['val_loss'])\n",
        "\n",
        "\n",
        "plt.ylabel('Perda')\n",
        "plt.xlabel('Época')\n",
        "plt.legend(['Treinamento', 'Validação'], loc = 'upper right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ySD4TmTP33L"
      },
      "outputs": [],
      "source": [
        "plt.plot(history['accuracy'])\n",
        "plt.plot(history['val_accuracy'])\n",
        "plt.title('Acurácia')\n",
        "plt.ylabel('Acurácia')\n",
        "plt.xlabel('Épocas')\n",
        "plt.legend(['Treinamento', 'Validação'], loc='upper left')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvrrbmVjFekv"
      },
      "source": [
        "7. Salvando o modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZKAgvZsP605"
      },
      "outputs": [],
      "source": [
        "preds = modelo_carregado.predict(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-72gQj5DP9X6"
      },
      "outputs": [],
      "source": [
        "#Função da Matriz de Confusão\n",
        "\n",
        "def plot_confusion_matrix(\n",
        "        cm,\n",
        "        classes,\n",
        "        normalize=False,\n",
        "        title='Confusion matrix',\n",
        "        cmap=plt.cm.Blues\n",
        "    ):\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    #plt.title(title)\n",
        "    #plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('Classe real')\n",
        "    plt.xlabel('Classe predita')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nTqr20FP-PP"
      },
      "outputs": [],
      "source": [
        "#Gráfico da Matriz de Confusão\n",
        "\n",
        "y_test_ = [np.argmax(x) for x in y_test]\n",
        "preds_ = [np.argmax(x) for x in preds]\n",
        "\n",
        "cm = confusion_matrix(y_test_, preds_)\n",
        "plot_confusion_matrix(cm, classes=['Crazing', 'Scratches','Inclusion'], title='Confusion matrix')\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54fo7piqQDKp"
      },
      "outputs": [],
      "source": [
        "#Resultado das predições\n",
        "\n",
        "n = 4\n",
        "for t in range(4):\n",
        "    plt.figure(figsize=(10,10))\n",
        "    for i in range(n*t, n*(t+1)):\n",
        "        plt.subplot(1, n, i + 1 - n*t)\n",
        "        plt.imshow(cv2.cvtColor(X_test[i], cv2.COLOR_BGR2RGB), cmap='gray')\n",
        "        plt.title('Real: {}\\nPredito: {}'.format(classes[np.argmax(y_test[i])], classes[np.argmax(preds[i])]))\n",
        "        plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Using the model to classify images**"
      ],
      "metadata": {
        "id": "4KUGeaIhLFQ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Load Libraries**"
      ],
      "metadata": {
        "id": "wJcNKH5GLStx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from google.colab import drive\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.image import img_to_array"
      ],
      "metadata": {
        "id": "aYTpMv0TLLwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Google Drive and Path**"
      ],
      "metadata": {
        "id": "zGguXh0iLZe5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load the pre-trained model\n",
        "model_path = '/content/drive/MyDrive/Pós-doutorado/Dataset_M_S_T_01/The_Model_Metal_Surfaces_v01/modelosalvo.keras' #'/content/drive/MyDrive/modelosalvo.keras'  # Update this path to match your model's location\n",
        "model = load_model(model_path)"
      ],
      "metadata": {
        "id": "Kpe_j-FfLcqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Labels**"
      ],
      "metadata": {
        "id": "Nf7Tfun5L4ug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class labels (update these to match your model's classes)\n",
        "class_labels = ['Crazing', 'Scratches','Inclusion']"
      ],
      "metadata": {
        "id": "Q2pLa4HeMFyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Preprocessing images**"
      ],
      "metadata": {
        "id": "yPPgXPCGMTT3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_image(image_path, target_size=(48, 48)):\n",
        "    \"\"\"Preprocess image for model input\"\"\"\n",
        "    img = Image.open(image_path).convert('RGB')\n",
        "    img = img.resize(target_size)\n",
        "    img_array = img_to_array(img)\n",
        "    img_array = img_array / 255.0  # Normalize pixel values\n",
        "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
        "    return img, img_array"
      ],
      "metadata": {
        "id": "rU_xuovdMeJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. **Classify Image**"
      ],
      "metadata": {
        "id": "k7S6wwMdQO_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_image(image_path):\n",
        "    \"\"\"Classify an image using the pre-trained model\"\"\"\n",
        "    # Preprocess image\n",
        "    original_img, processed_img = preprocess_image(image_path)\n",
        "\n",
        "    # Get prediction\n",
        "    prediction = model.predict(processed_img)\n",
        "    predicted_class_index = np.argmax(prediction[0])\n",
        "    predicted_class = class_labels[predicted_class_index]\n",
        "    confidence = prediction[0][predicted_class_index] * 100\n",
        "\n",
        "    # Display results\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(original_img)\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"This image is classified as: {predicted_class.upper()}\\nConfidence: {confidence:.2f}%\",\n",
        "              fontsize=16, pad=20)\n",
        "    plt.show()\n",
        "\n",
        "    return predicted_class, confidence\n"
      ],
      "metadata": {
        "id": "3-oMDe8aQQ_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Example Usage**"
      ],
      "metadata": {
        "id": "MvF9OD4eUkE_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "def classify_image_from_drive(relative_path):\n",
        "    \"\"\"Classify an image from Google Drive by providing a path relative to MyDrive\"\"\"\n",
        "    full_path = f'/content/drive/MyDrive/Pós-doutorado/Dataset_M_S_T_01/Crazing_test_image/{relative_path}'\n",
        "    if not os.path.exists(full_path):\n",
        "        print(f\"Error: File not found at {full_path}\")\n",
        "        return\n",
        "\n",
        "    print(f\"Processing image: {relative_path}\")\n",
        "    predicted_class, confidence = classify_image(full_path)\n",
        "    print(f\"Classification result: {predicted_class}\")\n",
        "    print(f\"Confidence: {confidence:.2f}%\")\n"
      ],
      "metadata": {
        "id": "-DK6mcqhUhKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. Function to let user input path**"
      ],
      "metadata": {
        "id": "Ro4P1PNKQmR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to let user input a path\n",
        "def classify_from_user_input():\n",
        "    image_path = input(\"Enter the path to your image relative to Google Drive root (e.g., 'metal_surface_model/In_1.bmp'): \")\n",
        "    classify_image_from_drive(image_path)\n",
        "\n",
        "# Run the interactive function\n",
        "classify_from_user_input()"
      ],
      "metadata": {
        "id": "97lga1ZPVV0V"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}