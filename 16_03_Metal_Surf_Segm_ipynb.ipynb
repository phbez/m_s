{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/phbez/m_s/blob/main/16_03_Metal_Surf_Segm_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpfAq_3AqbLm"
      },
      "source": [
        "1. Importação das bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "O-ELVWXkAQCb"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Core ML/DL imports\n",
        "#\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Conv2D, MaxPooling2D, Dense, Dropout, Flatten, \n",
        "    BatchNormalization, Activation, Add, Input, AveragePooling2D)\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import (\n",
        "    ImageDataGenerator, array_to_img, img_to_array, load_img)\n",
        "from tensorflow.keras import applications, regularizers\n",
        "\n",
        "from tensorflow.keras import optimizers, Input\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Data processing and visualization\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import ndimage\n",
        "import scipy.stats\n",
        "import itertools\n",
        "\n",
        "# Model evaluation imports\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix, ConfusionMatrixDisplay, \n",
        "    accuracy_score, precision_score, recall_score, f1_score)\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "\n",
        "# System imports\n",
        "import os\n",
        "import sys\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuuRcyf5zzcq"
      },
      "source": [
        "**1.1 Segment Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63QMM-SGz3rX",
        "outputId": "6f0e8f53-18e9-4fbd-b110-9a150c25228e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/facebookresearch/segment-anything.git\n",
            "  Cloning https://github.com/facebookresearch/segment-anything.git to c:\\users\\paulo\\appdata\\local\\temp\\pip-req-build-yrf2vocu\n",
            "  Resolved https://github.com/facebookresearch/segment-anything.git to commit dca509fe793f601edb92606367a655c15ac00fdf\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/segment-anything.git 'C:\\Users\\Paulo\\AppData\\Local\\Temp\\pip-req-build-yrf2vocu'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opencv-python in c:\\users\\paulo\\.ipython\\m_s\\phenvione\\lib\\site-packages (4.11.0.86)\n",
            "Requirement already satisfied: pycocotools in c:\\users\\paulo\\.ipython\\m_s\\phenvione\\lib\\site-packages (2.0.8)\n",
            "Requirement already satisfied: matplotlib in c:\\users\\paulo\\.ipython\\m_s\\phenvione\\lib\\site-packages (3.10.1)\n",
            "Requirement already satisfied: onnxruntime in c:\\users\\paulo\\.ipython\\m_s\\phenvione\\lib\\site-packages (1.21.0)\n",
            "Requirement already satisfied: onnx in c:\\users\\paulo\\.ipython\\m_s\\phenvione\\lib\\site-packages (1.17.0)\n",
            "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\paulo\\.ipython\\m_s\\phenvione\\lib\\site-packages (from opencv-python) (2.1.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\paulo\\.ipython\\m_s\\phenvione\\lib\\site-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\paulo\\.ipython\\m_s\\phenvione\\lib\\site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\paulo\\.ipython\\m_s\\phenvione\\lib\\site-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\paulo\\.ipython\\m_s\\phenvione\\lib\\site-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\paulo\\.ipython\\m_s\\phenvione\\lib\\site-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in c:\\users\\paulo\\.ipython\\m_s\\phenvione\\lib\\site-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\paulo\\.ipython\\m_s\\phenvione\\lib\\site-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\paulo\\.ipython\\m_s\\phenvione\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: coloredlogs in c:\\users\\paulo\\.ipython\\m_s\\phenvione\\lib\\site-packages (from onnxruntime) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in c:\\users\\paulo\\.ipython\\m_s\\phenvione\\lib\\site-packages (from onnxruntime) (25.2.10)\n",
            "Requirement already satisfied: protobuf in c:\\users\\paulo\\.ipython\\m_s\\phenvione\\lib\\site-packages (from onnxruntime) (5.29.3)\n",
            "Requirement already satisfied: sympy in c:\\users\\paulo\\.ipython\\m_s\\phenvione\\lib\\site-packages (from onnxruntime) (1.13.1)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\paulo\\.ipython\\m_s\\phenvione\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\paulo\\.ipython\\m_s\\phenvione\\lib\\site-packages (from coloredlogs->onnxruntime) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\paulo\\.ipython\\m_s\\phenvione\\lib\\site-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Requirement already satisfied: pyreadline3 in c:\\users\\paulo\\.ipython\\m_s\\phenvione\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime) (3.5.4)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "J� existe uma subpasta ou um arquivo -p.\n",
            "Erro ao processar: -p.\n",
            "J� existe uma subpasta ou um arquivo sam_models.\n",
            "Erro ao processar: sam_models.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint already exists at sam_models/sam_vit_b.pth\n"
          ]
        }
      ],
      "source": [
        "#####################################################################\n",
        "# SAM Model Setup\n",
        "!pip install git+https://github.com/facebookresearch/segment-anything.git\n",
        "!pip install opencv-python pycocotools matplotlib onnxruntime onnx\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import requests\n",
        "from segment_anything import sam_model_registry, SamPredictor, SamAutomaticMaskGenerator\n",
        "\n",
        "# Create a directory for the model\n",
        "!mkdir -p sam_models\n",
        "\n",
        "# Download SAM model checkpoint\n",
        "def download_sam_checkpoint(model_type='vit_b'):\n",
        "    checkpoint_urls = {\n",
        "        'vit_h': 'https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth',\n",
        "        'vit_l': 'https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth',\n",
        "        'vit_b': 'https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth'\n",
        "    }\n",
        "\n",
        "    checkpoint_path = f\"sam_models/sam_{model_type}.pth\"\n",
        "\n",
        "    if not os.path.exists(checkpoint_path):\n",
        "        print(f\"Downloading {model_type} checkpoint...\")\n",
        "        response = requests.get(checkpoint_urls[model_type])\n",
        "        with open(checkpoint_path, \"wb\") as f:\n",
        "            f.write(response.content)\n",
        "        print(f\"Downloaded {model_type} checkpoint to {checkpoint_path}\")\n",
        "    else:\n",
        "        print(f\"Checkpoint already exists at {checkpoint_path}\")\n",
        "\n",
        "    return checkpoint_path\n",
        "\n",
        "# Download and load the model\n",
        "model_type = 'vit_b'  # Use smaller model for faster processing\n",
        "checkpoint_path = download_sam_checkpoint(model_type)\n",
        "sam = sam_model_registry[model_type](checkpoint=checkpoint_path)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "sam.to(device)\n",
        "\n",
        "# Create SAM predictor and mask generator\n",
        "predictor = SamPredictor(sam)\n",
        "mask_generator = SamAutomaticMaskGenerator(\n",
        "    model=sam,\n",
        "    points_per_side=32,\n",
        "    pred_iou_thresh=0.85,\n",
        "    stability_score_thresh=0.9,\n",
        "    crop_n_layers=1,\n",
        "    crop_n_points_downscale_factor=2,\n",
        "    min_mask_region_area=100\n",
        ")\n",
        "#####################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVShPA5kEb1h"
      },
      "source": [
        "**2. Loading dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97FEPpcSBGiU",
        "outputId": "2ae9068e-93e7-432a-8c39-c4aceacf0a03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gdown in c:\\users\\paulo\\.ipython\\m_s\\phenvione\\lib\\site-packages (5.2.0)\n",
            "Requirement already satisfied: pandas in c:\\users\\paulo\\.ipython\\m_s\\phenvione\\lib\\site-packages (2.2.3)\n",
            "Requirement already satisfied: numpy in c:\\users\\paulo\\.ipython\\m_s\\phenvione\\lib\\site-packages (2.1.3)\n",
            "Requirement already satisfied: beautifulsoup4 in c:\\users\\paulo\\.ipython\\m_s\\phenvione\\lib\\site-packages (from gdown) (4.13.3)\n",
            "Requirement already satisfied: filelock in c:\\users\\paulo\\.ipython\\m_s\\phenvione\\lib\\site-packages (from gdown) (3.18.0)\n",
            "Requirement already satisfied: requests[socks] in c:\\users\\paulo\\.ipython\\m_s\\phenvione\\lib\\site-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in c:\\users\\paulo\\.ipython\\m_s\\phenvione\\lib\\site-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\paulo\\.ipython\\m_s\\phenvione\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\paulo\\.ipython\\m_s\\phenvione\\lib\\site-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\paulo\\.ipython\\m_s\\phenvione\\lib\\site-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\paulo\\.ipython\\m_s\\phenvione\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in c:\\users\\paulo\\.ipython\\m_s\\phenvione\\lib\\site-packages (from beautifulsoup4->gdown) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\paulo\\.ipython\\m_s\\phenvione\\lib\\site-packages (from beautifulsoup4->gdown) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\paulo\\.ipython\\m_s\\phenvione\\lib\\site-packages (from requests[socks]->gdown) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\paulo\\.ipython\\m_s\\phenvione\\lib\\site-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\paulo\\.ipython\\m_s\\phenvione\\lib\\site-packages (from requests[socks]->gdown) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\paulo\\.ipython\\m_s\\phenvione\\lib\\site-packages (from requests[socks]->gdown) (2025.1.31)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\paulo\\.ipython\\m_s\\phenvione\\lib\\site-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\paulo\\.ipython\\m_s\\phenvione\\lib\\site-packages (from tqdm->gdown) (0.4.6)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=16-pPc_hHPbDLxnNE0gR88zhuKSJ6vGz4\n",
            "To: c:\\Users\\Paulo\\.ipython\\m_s\\data\\dataset\\dataset.zip\n",
            "100%|██████████| 24.6M/24.6M [00:18<00:00, 1.35MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloaded and extracted to c:\\Users\\Paulo\\.ipython\\m_s\\data\\dataset\n"
          ]
        }
      ],
      "source": [
        "!pip install gdown pandas numpy\n",
        "\n",
        "import os\n",
        "import gdown\n",
        "import zipfile\n",
        "\n",
        "# Create data directories\n",
        "DATA_DIR = os.path.join(os.getcwd(), 'data')\n",
        "DATASET_DIR = os.path.join(DATA_DIR, 'dataset')\n",
        "os.makedirs(DATASET_DIR, exist_ok=True)\n",
        "\n",
        "# Google Drive ZIP file ID (replace with the ZIP file ID)\n",
        "zip_file_id = '16-pPc_hHPbDLxnNE0gR88zhuKSJ6vGz4' #Replace this with your zip file id\n",
        "\n",
        "output_path = os.path.join(DATASET_DIR, 'dataset.zip')\n",
        "\n",
        "# Download from Google Drive\n",
        "url = f'https://drive.google.com/uc?id={zip_file_id}'\n",
        "gdown.download(url, output_path, quiet=False)\n",
        "\n",
        "# Unzip the downloaded file\n",
        "with zipfile.ZipFile(output_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(DATASET_DIR)\n",
        "\n",
        "print(f\"Downloaded and extracted to {DATASET_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Jif1DKcqC7n5"
      },
      "outputs": [],
      "source": [
        "#folder='/content/drive/MyDrive/Pós-doutorado/Dataset_M_S_T_01'\n",
        "\n",
        "folder = os.path.join(DATASET_DIR) # DATASET_DIR was defined in the prior cell."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJqawfUWEk61"
      },
      "source": [
        "3. Estruturando o dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "U_RoGeP5D-Wl",
        "outputId": "9b5b299e-a110-4a2c-bef9-97a2786eab85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Paulo\\.ipython\\m_s\\data\\dataset\\Crazing\n",
            "c:\\Users\\Paulo\\.ipython\\m_s\\data\\dataset\\Scratches\n",
            "c:\\Users\\Paulo\\.ipython\\m_s\\data\\dataset\\Inclusion\n",
            "Processing images with SAM...\n",
            "Processing image 1/828\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 58\u001b[39m\n\u001b[32m     55\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProcessing image \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_files)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# Process image with SAM\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m processed_image = \u001b[43mprocess_with_sam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# Normalize and add to dataset\u001b[39;00m\n\u001b[32m     61\u001b[39m dataset[idx] = processed_image / \u001b[32m255.0\u001b[39m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mprocess_with_sam\u001b[39m\u001b[34m(image_path)\u001b[39m\n\u001b[32m     24\u001b[39m image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Generate masks for the image\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m masks = \u001b[43mmask_generator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# If no masks found, return the original image resized\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m masks:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Paulo\\.ipython\\m_s\\phenvione\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Paulo\\.ipython\\m_s\\phenvione\\Lib\\site-packages\\segment_anything\\automatic_mask_generator.py:163\u001b[39m, in \u001b[36mSamAutomaticMaskGenerator.generate\u001b[39m\u001b[34m(self, image)\u001b[39m\n\u001b[32m    138\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    139\u001b[39m \u001b[33;03mGenerates masks for the given image.\u001b[39;00m\n\u001b[32m    140\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    159\u001b[39m \u001b[33;03m         the mask, given in XYWH format.\u001b[39;00m\n\u001b[32m    160\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    162\u001b[39m \u001b[38;5;66;03m# Generate masks\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m mask_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_masks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[38;5;66;03m# Filter small disconnected regions and holes in masks\u001b[39;00m\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.min_mask_region_area > \u001b[32m0\u001b[39m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Paulo\\.ipython\\m_s\\phenvione\\Lib\\site-packages\\segment_anything\\automatic_mask_generator.py:206\u001b[39m, in \u001b[36mSamAutomaticMaskGenerator._generate_masks\u001b[39m\u001b[34m(self, image)\u001b[39m\n\u001b[32m    204\u001b[39m data = MaskData()\n\u001b[32m    205\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m crop_box, layer_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(crop_boxes, layer_idxs):\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m     crop_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_crop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrop_box\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morig_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    207\u001b[39m     data.cat(crop_data)\n\u001b[32m    209\u001b[39m \u001b[38;5;66;03m# Remove duplicate masks between crops\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Paulo\\.ipython\\m_s\\phenvione\\Lib\\site-packages\\segment_anything\\automatic_mask_generator.py:236\u001b[39m, in \u001b[36mSamAutomaticMaskGenerator._process_crop\u001b[39m\u001b[34m(self, image, crop_box, crop_layer_idx, orig_size)\u001b[39m\n\u001b[32m    234\u001b[39m cropped_im = image[y0:y1, x0:x1, :]\n\u001b[32m    235\u001b[39m cropped_im_size = cropped_im.shape[:\u001b[32m2\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredictor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcropped_im\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[38;5;66;03m# Get points for this crop\u001b[39;00m\n\u001b[32m    239\u001b[39m points_scale = np.array(cropped_im_size)[\u001b[38;5;28;01mNone\u001b[39;00m, ::-\u001b[32m1\u001b[39m]\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Paulo\\.ipython\\m_s\\phenvione\\Lib\\site-packages\\segment_anything\\predictor.py:60\u001b[39m, in \u001b[36mSamPredictor.set_image\u001b[39m\u001b[34m(self, image, image_format)\u001b[39m\n\u001b[32m     57\u001b[39m input_image_torch = torch.as_tensor(input_image, device=\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m     58\u001b[39m input_image_torch = input_image_torch.permute(\u001b[32m2\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m).contiguous()[\u001b[38;5;28;01mNone\u001b[39;00m, :, :, :]\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mset_torch_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_image_torch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Paulo\\.ipython\\m_s\\phenvione\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Paulo\\.ipython\\m_s\\phenvione\\Lib\\site-packages\\segment_anything\\predictor.py:89\u001b[39m, in \u001b[36mSamPredictor.set_torch_image\u001b[39m\u001b[34m(self, transformed_image, original_image_size)\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;28mself\u001b[39m.input_size = \u001b[38;5;28mtuple\u001b[39m(transformed_image.shape[-\u001b[32m2\u001b[39m:])\n\u001b[32m     88\u001b[39m input_image = \u001b[38;5;28mself\u001b[39m.model.preprocess(transformed_image)\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m \u001b[38;5;28mself\u001b[39m.features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimage_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[38;5;28mself\u001b[39m.is_image_set = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Paulo\\.ipython\\m_s\\phenvione\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Paulo\\.ipython\\m_s\\phenvione\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Paulo\\.ipython\\m_s\\phenvione\\Lib\\site-packages\\segment_anything\\modeling\\image_encoder.py:112\u001b[39m, in \u001b[36mImageEncoderViT.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    109\u001b[39m     x = x + \u001b[38;5;28mself\u001b[39m.pos_embed\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.blocks:\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m     x = \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    114\u001b[39m x = \u001b[38;5;28mself\u001b[39m.neck(x.permute(\u001b[32m0\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m))\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Paulo\\.ipython\\m_s\\phenvione\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Paulo\\.ipython\\m_s\\phenvione\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Paulo\\.ipython\\m_s\\phenvione\\Lib\\site-packages\\segment_anything\\modeling\\image_encoder.py:180\u001b[39m, in \u001b[36mBlock.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    177\u001b[39m     x = window_unpartition(x, \u001b[38;5;28mself\u001b[39m.window_size, pad_hw, (H, W))\n\u001b[32m    179\u001b[39m x = shortcut + x\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m x = x + \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Paulo\\.ipython\\m_s\\phenvione\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Paulo\\.ipython\\m_s\\phenvione\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Paulo\\.ipython\\m_s\\phenvione\\Lib\\site-packages\\segment_anything\\modeling\\common.py:26\u001b[39m, in \u001b[36mMLPBlock.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor) -> torch.Tensor:\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlin2\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlin1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "#image resolution\n",
        "image_width = 48\n",
        "image_height = 48\n",
        "channels = 3 #rgb\n",
        "\n",
        "#####################################################################\n",
        "train_files = []\n",
        "classes_list = ['Crazing', 'Scratches', 'Inclusion']\n",
        "class_to_index = {cls: i for i, cls in enumerate(classes_list)}\n",
        "\n",
        "for cls in classes_list:\n",
        "    print(os.path.join(folder, cls))\n",
        "    onlyfiles = [f for f in os.listdir(os.path.join(folder, cls)) if os.path.isfile(os.path.join(folder, cls, f))]\n",
        "    for _file in onlyfiles:\n",
        "        train_files.append((os.path.join(folder, cls, _file), cls))\n",
        "\n",
        "# Function to process image with SAM\n",
        "def process_with_sam(image_path):\n",
        "    # Read the image\n",
        "    image = cv2.imread(image_path)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Generate masks for the image\n",
        "    masks = mask_generator.generate(image)\n",
        "\n",
        "    # If no masks found, return the original image resized\n",
        "    if not masks:\n",
        "        return cv2.resize(image, (image_width, image_height))\n",
        "\n",
        "    # Sort masks by area (largest first)\n",
        "    masks = sorted(masks, key=lambda x: x['area'], reverse=True)\n",
        "\n",
        "    # Get the largest mask (likely the main defect)\n",
        "    mask = masks[0]['segmentation']\n",
        "\n",
        "    # Apply the mask to the original image\n",
        "    masked_image = image.copy()\n",
        "    masked_image[~mask] = 0  # Set background to black\n",
        "\n",
        "    # Resize to the required dimensions\n",
        "    masked_image = cv2.resize(masked_image, (image_width, image_height))\n",
        "\n",
        "    return masked_image\n",
        "\n",
        "# Process and load the dataset\n",
        "dataset = np.zeros((len(train_files), image_height, image_width, channels), dtype=np.float32)\n",
        "y_dataset = []\n",
        "\n",
        "print(\"Processing images with SAM...\")\n",
        "for idx, (file_path, cls) in enumerate(train_files):\n",
        "    if idx % 10 == 0:\n",
        "        print(f\"Processing image {idx+1}/{len(train_files)}\")\n",
        "\n",
        "    # Process image with SAM\n",
        "    processed_image = process_with_sam(file_path)\n",
        "\n",
        "    # Normalize and add to dataset\n",
        "    dataset[idx] = processed_image / 255.0\n",
        "\n",
        "    # Add class label\n",
        "    y_dataset.append(class_to_index[cls])\n",
        "\n",
        "# Convert labels to categorical\n",
        "y_dataset = to_categorical(y_dataset, num_classes=len(classes_list))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IX_rk-gqH20K"
      },
      "outputs": [],
      "source": [
        "#i = 0\n",
        "#for classes in ['Crazing', 'Scratches','Inclusion']:\n",
        "    #onlyfiles = [f for f in os.listdir(os.path.join(folder, classes)) if os.path.isfile(os.path.join(folder, classes, f))]\n",
        "    #for _file in onlyfiles:\n",
        "        #img_path = os.path.join(folder, classes, _file)\n",
        "        #img = load_img(img_path, target_size=(image_height, image_width))\n",
        "        #x = img_to_array(img)\n",
        "        #dataset[i] = x\n",
        "        #mapping = {'Crazing': 0 , 'Scratches': 1, 'Inclusion': 2}\n",
        "        #y_dataset.append(mapping[classes])\n",
        "        #i += 1\n",
        "        #if i == 30000:\n",
        "        #    print(\"%d images to array\" % i)\n",
        "        #    break\n",
        "\n",
        "print(\"All images to array!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4PABmo4i083"
      },
      "source": [
        "6. Normalizando os dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMVib39IK1hK"
      },
      "outputs": [],
      "source": [
        "#Normalização\n",
        "#dataset = dataset.astype('float32')\n",
        "#dataset /= 255"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NXJK8iv97qB"
      },
      "source": [
        "**3.1 Visualize some examples with SAM segmentation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQFH06Im-GnU"
      },
      "outputs": [],
      "source": [
        "# Add this after your dataset loading to visualize some examples\n",
        "#####################################################################\n",
        "# Visualize some examples with SAM segmentation\n",
        "def visualize_sam_examples(num_examples=5):\n",
        "    plt.figure(figsize=(15, 5*num_examples))\n",
        "\n",
        "    for i in range(min(num_examples, len(train_files))):\n",
        "        # Get a random sample\n",
        "        idx = np.random.randint(0, len(train_files))\n",
        "        file_path, cls = train_files[idx]\n",
        "\n",
        "        # Original image\n",
        "        original = cv2.imread(file_path)\n",
        "        original = cv2.cvtColor(original, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Process with SAM\n",
        "        processed = process_with_sam(file_path)\n",
        "\n",
        "        # Display\n",
        "        plt.subplot(num_examples, 2, i*2+1)\n",
        "        plt.imshow(original)\n",
        "        plt.title(f\"Original - Class: {cls}\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(num_examples, 2, i*2+2)\n",
        "        plt.imshow(processed)\n",
        "        plt.title(f\"SAM Segmented - Class: {cls}\")\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Visualize some examples\n",
        "visualize_sam_examples()\n",
        "#####################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZ7Ad5VeK3EK"
      },
      "outputs": [],
      "source": [
        "classes = ['Crazing', 'Scratches', 'Inclusion']\n",
        "\n",
        "# Dicionário para armazenar o índice da primeira imagem de cada classe\n",
        "first_image_index = {}\n",
        "\n",
        "# Encontra o índice da primeira imagem de cada classe\n",
        "for i, label in enumerate(y_dataset):\n",
        "    if label not in first_image_index:\n",
        "        first_image_index[label] = i\n",
        "\n",
        "# Configura a grade para exibir as imagens\n",
        "num_classes = len(set(y_dataset))\n",
        "num_images_per_class = 1\n",
        "num_cols = num_classes\n",
        "num_rows = num_images_per_class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7jaYKd-La09"
      },
      "outputs": [],
      "source": [
        "# Cria uma figura com uma grade de subplots\n",
        "fig, axes = plt.subplots(num_rows, num_cols, figsize=(5, 15))\n",
        "\n",
        "# Loop através de cada classe\n",
        "for i in range(num_classes):\n",
        "    # Obtém o índice da primeira imagem da classe\n",
        "    idx = first_image_index[i]\n",
        "\n",
        "    # Obtém a imagem e converte para RGB\n",
        "    pixels = dataset[idx]\n",
        "\n",
        "    # Exibe a imagem no subplot correspondente\n",
        "    axes[i].imshow(pixels, cmap='Spectral')\n",
        "    axes[i].axis('off')\n",
        "\n",
        "    # Adiciona um título para o subplot com o rótulo\n",
        "    axes[i].set_title(f'{classes[i]}')\n",
        "\n",
        "# Exibe a figura\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgawN2E0qN2x"
      },
      "source": [
        "7. Normalizando o dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2v4Yj4vOcmw"
      },
      "outputs": [],
      "source": [
        "# categorical values\n",
        "n_classes = len(set(y_dataset))\n",
        "print(n_classes)\n",
        "\n",
        "y_dataset_ = to_categorical(y_dataset, n_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4krsLVb4OiXA"
      },
      "outputs": [],
      "source": [
        "# Ajusta o tamanho do dataset para coincidir com o tamanho menor entre dataset_trimmed e y_dataset_\n",
        "min_length = min(len(dataset), len(y_dataset_))\n",
        "dataset_trimmed = dataset[:min_length]\n",
        "y_dataset_trimmed = y_dataset_[:min_length]\n",
        "\n",
        "# Verifica se os tamanhos agora estão iguais\n",
        "assert len(dataset_trimmed) == len(y_dataset_trimmed), \"Os tamanhos ainda não são consistentes!\"\n",
        "\n",
        "# Dividindo em conjuntos de treino e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(dataset_trimmed, y_dataset_trimmed, test_size=0.2)\n",
        "\n",
        "print(\"Train set size: {}, Test set size: {}\".format(len(X_train), len(X_test)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nr-kbaYcOp1U"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils import resample\n",
        "\n",
        "# Criar listas vazias para armazenar as amostras balanceadas\n",
        "balanced_X_train = []\n",
        "balanced_y_train = []\n",
        "\n",
        "# Determinar o número de amostras na classe majoritária\n",
        "majority_samples = 5000\n",
        "\n",
        "# Iterar sobre cada classe\n",
        "for class_label in np.unique(y_train.argmax(axis=1)):\n",
        "    # Filtrar amostras pertencentes a essa classe\n",
        "    X_class = X_train[y_train.argmax(axis=1) == class_label]\n",
        "    y_class = y_train[y_train.argmax(axis=1) == class_label]\n",
        "\n",
        "    # Calcular o número de amostras na classe menos representada\n",
        "    minority_samples = len(X_class)\n",
        "\n",
        "    # Balancear as amostras aumentando a classe menos representada\n",
        "    balanced_X_class, balanced_y_class = resample(X_class, y_class,\n",
        "                                                  replace=True,\n",
        "                                                  n_samples=majority_samples,\n",
        "                                                  random_state=42)\n",
        "\n",
        "    # Adicionar amostras balanceadas à lista\n",
        "    balanced_X_train.extend(balanced_X_class)\n",
        "    balanced_y_train.extend(balanced_y_class)\n",
        "\n",
        "# Converter listas em arrays numpy\n",
        "balanced_X_train = np.array(balanced_X_train)\n",
        "balanced_y_train = np.array(balanced_y_train)\n",
        "\n",
        "# Embaralhar amostras\n",
        "shuffled_indices = np.arange(len(balanced_X_train))\n",
        "np.random.shuffle(shuffled_indices)\n",
        "balanced_X_train = balanced_X_train[shuffled_indices]\n",
        "balanced_y_train = balanced_y_train[shuffled_indices]\n",
        "\n",
        "# Verificar o tamanho dos conjuntos de dados balanceados\n",
        "print(\"Tamanho do conjunto de treinamento balanceado:\", len(balanced_X_train))\n",
        "print(\"Tamanho do conjunto de teste:\", len(X_test))\n",
        "\n",
        "for class_label in np.unique(balanced_y_train.argmax(axis=1)):\n",
        "    count = np.sum(balanced_y_train.argmax(axis=1) == class_label)\n",
        "    print(f\"Classe {class_label}: {count} amostras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZorQkRrSOwWP"
      },
      "outputs": [],
      "source": [
        "# Criar o modelo\n",
        "model = Sequential()\n",
        "\n",
        "model.add(BatchNormalization(input_shape=(image_height, image_width, 3)))\n",
        "model.add(Conv2D(512, (3,3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "model.add(Dropout(0.2))  # Adiciona a camada de dropout\n",
        "\n",
        "model.add(Conv2D(256, (3,3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "model.add(Dropout(0.2))  # Adiciona a camada de dropout\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dropout(0.2))  # Adiciona a camada de dropout\n",
        "#model.add(Dense(2, activation='softmax'))  # Especifica 'softmax' como a função de ativação\n",
        "model.add(Dense(3, activation='softmax'))  # Especifica 'softmax' como a função de ativação\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xo_RERv1RhSR"
      },
      "source": [
        "4. Compilando o modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpodQ283O6u4"
      },
      "outputs": [],
      "source": [
        "optimizer = Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78PbJ9QSPAwq"
      },
      "outputs": [],
      "source": [
        "# Treinando o modelo\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Configurar EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=15)\n",
        "\n",
        "# Treinar o modelo\n",
        "history = model.fit(balanced_X_train, balanced_y_train,validation_split= 0.2, epochs=100, callbacks=[early_stopping], batch_size=64)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knlJ3ziiPkFX"
      },
      "outputs": [],
      "source": [
        "val_accuracy = history.history['val_accuracy']\n",
        "\n",
        "mean_val_accuracy = np.mean(val_accuracy)\n",
        "\n",
        "print(\"Valor médio de acurácia:\", mean_val_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wP1qB1BKFO5f"
      },
      "source": [
        "5. Carregando o modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8jkc_ryPoP5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7v28nYJ0q0Y7"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive (add this line)\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMv2uBUMq5kl"
      },
      "outputs": [],
      "source": [
        "# START OF SAVING CODE\n",
        "# Save training history\n",
        "history_save = pd.DataFrame(history.history)\n",
        "#history_save.to_csv('datasetNormal.csv') # old: temporarily saving\n",
        "history_save.to_csv('/content/drive/MyDrive/Pós-doutorado/Dataset_M_S_T_01/The_Model_Metal_Surfaces_v01/datasetNormal.csv')  # Changed path to Google Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQFa4na5ukEi"
      },
      "source": [
        "**Save model structure in JSON file**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RLiMvuRPq4q"
      },
      "outputs": [],
      "source": [
        "model_json = model.to_json()\n",
        "#with open(\"metal_surface_model.json\", \"w\") as json_file: # old: temporarily saving\n",
        "with open(\"/content/drive/MyDrive/Pós-doutorado/Dataset_M_S_T_01/The_Model_Metal_Surfaces_v01/metal_surface_model.json\", \"w\") as json_file:  # Changed path\n",
        "    json_file.write(model_json)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNqDDb3ruayF"
      },
      "source": [
        "**Save complete model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1NHVdRLPtD_"
      },
      "outputs": [],
      "source": [
        "#model.save('modelosalvo.keras') # old: temporarily saving\n",
        "model.save('/content/drive/MyDrive/Pós-doutorado/Dataset_M_S_T_01/The_Model_Metal_Surfaces_v01/modelosalvo.keras')  # Changed path\n",
        "# END OF SAVING CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xq0Kctrnu33p"
      },
      "source": [
        "**Load model (for verification or immediate use)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VnZg02iEPvam"
      },
      "outputs": [],
      "source": [
        "from keras.models import load_model\n",
        "modelo_carregado = load_model('/content/drive/MyDrive/Pós-doutorado/Dataset_M_S_T_01/The_Model_Metal_Surfaces_v01/modelosalvo.keras')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIv4CoHlvckk"
      },
      "source": [
        "**Load history**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LpmHXjmgPxyV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "history = pd.read_csv('/content/drive/MyDrive/Pós-doutorado/Dataset_M_S_T_01/The_Model_Metal_Surfaces_v01/datasetNormal.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXD1EO4ZFXLp"
      },
      "source": [
        "6. Plotando o modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R91qUmtGP16D"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history['loss'])\n",
        "plt.plot(history['val_loss'])\n",
        "\n",
        "\n",
        "plt.ylabel('Perda')\n",
        "plt.xlabel('Época')\n",
        "plt.legend(['Treinamento', 'Validação'], loc = 'upper right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ySD4TmTP33L"
      },
      "outputs": [],
      "source": [
        "plt.plot(history['accuracy'])\n",
        "plt.plot(history['val_accuracy'])\n",
        "plt.title('Acurácia')\n",
        "plt.ylabel('Acurácia')\n",
        "plt.xlabel('Épocas')\n",
        "plt.legend(['Treinamento', 'Validação'], loc='upper left')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvrrbmVjFekv"
      },
      "source": [
        "7. Salvando o modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZKAgvZsP605"
      },
      "outputs": [],
      "source": [
        "preds = modelo_carregado.predict(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-72gQj5DP9X6"
      },
      "outputs": [],
      "source": [
        "#Função da Matriz de Confusão\n",
        "\n",
        "def plot_confusion_matrix(\n",
        "        cm,\n",
        "        classes,\n",
        "        normalize=False,\n",
        "        title='Confusion matrix',\n",
        "        cmap=plt.cm.Blues\n",
        "    ):\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    #plt.title(title)\n",
        "    #plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('Classe real')\n",
        "    plt.xlabel('Classe predita')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nTqr20FP-PP"
      },
      "outputs": [],
      "source": [
        "#Gráfico da Matriz de Confusão\n",
        "\n",
        "y_test_ = [np.argmax(x) for x in y_test]\n",
        "preds_ = [np.argmax(x) for x in preds]\n",
        "\n",
        "cm = confusion_matrix(y_test_, preds_)\n",
        "plot_confusion_matrix(cm, classes=['Crazing', 'Scratches','Inclusion'], title='Confusion matrix')\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54fo7piqQDKp"
      },
      "outputs": [],
      "source": [
        "#Resultado das predições\n",
        "\n",
        "n = 4\n",
        "for t in range(4):\n",
        "    plt.figure(figsize=(10,10))\n",
        "    for i in range(n*t, n*(t+1)):\n",
        "        plt.subplot(1, n, i + 1 - n*t)\n",
        "        plt.imshow(cv2.cvtColor(X_test[i], cv2.COLOR_BGR2RGB), cmap='gray')\n",
        "        plt.title('Real: {}\\nPredito: {}'.format(classes[np.argmax(y_test[i])], classes[np.argmax(preds[i])]))\n",
        "        plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KUGeaIhLFQ0"
      },
      "source": [
        "# **Using the model to classify images**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJcNKH5GLStx"
      },
      "source": [
        "**1. Load Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYTpMv0TLLwn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from google.colab import drive\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.image import img_to_array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGguXh0iLZe5"
      },
      "source": [
        "**2. Google Drive and Path**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kpe_j-FfLcqR"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load the pre-trained model\n",
        "model_path = '/content/drive/MyDrive/Pós-doutorado/Dataset_M_S_T_01/The_Model_Metal_Surfaces_v01/modelosalvo.keras' #'/content/drive/MyDrive/modelosalvo.keras'  # Update this path to match your model's location\n",
        "model = load_model(model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nf7Tfun5L4ug"
      },
      "source": [
        "**3. Labels**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2pLa4HeMFyt"
      },
      "outputs": [],
      "source": [
        "# Define class labels (update these to match your model's classes)\n",
        "class_labels = ['Crazing', 'Scratches','Inclusion']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPPgXPCGMTT3"
      },
      "source": [
        "**4. Preprocessing images**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rU_xuovdMeJw"
      },
      "outputs": [],
      "source": [
        "def preprocess_image(image_path, target_size=(48, 48)):\n",
        "    \"\"\"Preprocess image for model input\"\"\"\n",
        "    img = Image.open(image_path).convert('RGB')\n",
        "    img = img.resize(target_size)\n",
        "    img_array = img_to_array(img)\n",
        "    img_array = img_array / 255.0  # Normalize pixel values\n",
        "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
        "    return img, img_array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7S6wwMdQO_Z"
      },
      "source": [
        "5. **Classify Image**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-oMDe8aQQ_s"
      },
      "outputs": [],
      "source": [
        "def classify_image(image_path):\n",
        "    \"\"\"Classify an image using the pre-trained model\"\"\"\n",
        "    # Preprocess image\n",
        "    original_img, processed_img = preprocess_image(image_path)\n",
        "\n",
        "    # Get prediction\n",
        "    prediction = model.predict(processed_img)\n",
        "    predicted_class_index = np.argmax(prediction[0])\n",
        "    predicted_class = class_labels[predicted_class_index]\n",
        "    confidence = prediction[0][predicted_class_index] * 100\n",
        "\n",
        "    # Display results\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(original_img)\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"This image is classified as: {predicted_class.upper()}\\nConfidence: {confidence:.2f}%\",\n",
        "              fontsize=16, pad=20)\n",
        "    plt.show()\n",
        "\n",
        "    return predicted_class, confidence\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvF9OD4eUkE_"
      },
      "source": [
        "**6. Example Usage**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-DK6mcqhUhKI"
      },
      "outputs": [],
      "source": [
        "# Example usage\n",
        "def classify_image_from_drive(relative_path):\n",
        "    \"\"\"Classify an image from Google Drive by providing a path relative to MyDrive\"\"\"\n",
        "    full_path = f'/content/drive/MyDrive/Pós-doutorado/Dataset_M_S_T_01/Crazing_test_image/{relative_path}'\n",
        "    if not os.path.exists(full_path):\n",
        "        print(f\"Error: File not found at {full_path}\")\n",
        "        return\n",
        "\n",
        "    print(f\"Processing image: {relative_path}\")\n",
        "    predicted_class, confidence = classify_image(full_path)\n",
        "    print(f\"Classification result: {predicted_class}\")\n",
        "    print(f\"Confidence: {confidence:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ro4P1PNKQmR7"
      },
      "source": [
        "**7. Function to let user input path**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97lga1ZPVV0V"
      },
      "outputs": [],
      "source": [
        "# Function to let user input a path\n",
        "def classify_from_user_input():\n",
        "    image_path = input(\"Enter the path to your image relative to Google Drive root (e.g., 'metal_surface_model/In_1.bmp'): \")\n",
        "    classify_image_from_drive(image_path)\n",
        "\n",
        "# Run the interactive function\n",
        "classify_from_user_input()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "phenvione",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
